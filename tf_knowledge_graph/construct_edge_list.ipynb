{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the TF-TF network edge list\n",
    "This is the first of three notebooks required to build a TF-TF interaction network compatible with the KG UI. Here, we build an initial edge list of TF-TF interactions by performing ChEA3 transcription factor enrichment analysis (TFEA) on RummaGEO gene sets. \n",
    "\n",
    "After finishing this notebook, users should filter the network using `filter_assertions.ipynb`. \n",
    "## Set-Up \n",
    "First, import required packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyenrichr as py\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import warnings\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define our raw data and output directories, and create them if they don't already exist: \n",
    "1. `raw_data` holds all raw data files. We'll also add one subdirectory for the TFEA libraries, `chea3libs`\n",
    "2. `edge_constructing_files` will store our output files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = './raw_data'\n",
    "output = './edge_constructing_files'\n",
    "\n",
    "if not os.path.exists(raw_data):\n",
    "    os.mkdir(raw_data)\n",
    "    os.mkdir(f'{raw_data}/chea3libs')\n",
    "if not os.path.exists(output):\n",
    "    os.mkdir(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFEA using RummaGEO sets\n",
    "### Downloading required input data\n",
    "TF enrichment analysis for each of the 171k RummaGEO gene sets is performed using a local version of ChEA3.\n",
    "This requires files from two locations:\n",
    "1. RummaGEO:\n",
    "- Navigate to https://rummageo.com/, then click on downloads. Click on `human-geo-auto.gmt.gz` to download all human gene sets.\n",
    "- Unzip the file and move it to `raw_data`\n",
    "2. ChEA3: \n",
    "- Navigate to https://maayanlab.cloud/chea3/. Click on 'downloads' in the top right\n",
    "- Download the following six files (all of the files with file type `primary`) by clicking on their names:\n",
    "    1. ARCHS4_Coexpression.gmt\n",
    "    2. ENCODE_ChIP-seq.gmt\n",
    "    3. Enrichr_Queries.gmt\n",
    "    4. GTEx_Coexpression.gmt\n",
    "    5. Literature_ChIP-seq.gmt\n",
    "    6. ReMap_ChIP-seq.gmt\n",
    "- Move all six files to `raw_data/chea3libs`\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the mean ranks matrix\n",
    "Define functions for uploading a gmt file as a dictionary, and to divide a large dictionary into N smaller subdictionaries (this will help us process the RummaGEO file). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read a GMT file into a dictionary: this is formatted depending on the gmt being uploaded, specified by 'mode' \n",
    "def read_gmt_file(path, mode):\n",
    "  gmt = {}\n",
    "  print(\"Reading {}\".format(path))\n",
    "  with open(path, \"r\") as file:\n",
    "    # Read a tab-delimited GMT formatted as \"term gene1 gene2 ... geneN\" and check for set size\n",
    "    if mode == 'raw':\n",
    "      for line in tqdm.tqdm(file):\n",
    "        parts = line.strip().split('\\t')\n",
    "        if len(parts) > 5:\n",
    "          gmt[parts[0].strip()] = set(parts[1:])\n",
    "\n",
    "    # Read a GMT file formatted as \"signature id tf1 tf2 tf3 ... tf10\"\n",
    "    elif mode == 'signed':\n",
    "      for line in tqdm.tqdm(file):\n",
    "        signature, id, *tf = line.strip().split()\n",
    "        gmt[\" \".join([signature, id])] = set(tf)\n",
    "\n",
    "    # similar to mode = 'raw'\n",
    "    elif mode == 'transpose':\n",
    "      for line in tqdm.tqdm(file):\n",
    "        tf, *signature = line.strip().split()\n",
    "        gmt[tf] = set(signature)\n",
    "           \n",
    "  return gmt\n",
    "\n",
    "# divides a dictionary into N subdictionaries\n",
    "def split_dict(original_dict, N):\n",
    "    iter_dict = iter(original_dict.items())\n",
    "    list_of_dicts = []\n",
    "    \n",
    "    while True:\n",
    "        small_dict = {}\n",
    "        \n",
    "        try:\n",
    "            for _ in range(N):\n",
    "                key, value = next(iter_dict)\n",
    "                small_dict[key] = value\n",
    "        except StopIteration:\n",
    "            if small_dict:\n",
    "                list_of_dicts.append(small_dict)\n",
    "            break\n",
    "\n",
    "        list_of_dicts.append(small_dict)\n",
    "    \n",
    "    return list_of_dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import RummaGEO data and split into subdictionaries, to help with processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ./raw_data/human-geo-auto.gmt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "171441it [00:29, 5889.63it/s] \n"
     ]
    }
   ],
   "source": [
    "rummageo = f\"{raw_data}/human-geo-auto.gmt\"\n",
    "n_subdicts = 1000\n",
    "geo = read_gmt_file(rummageo, mode='raw')\n",
    "geo_split = split_dict(geo, n_subdicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each gene set, rank TFs based on enrichment and record the mean rank across ChEA3 libraries.\\\n",
    "**This step can take several hours.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the mean rank of a TF across all ChEA3 libraries\n",
    "def mean_rank(results):\n",
    "    if results:\n",
    "        # Extract gene set names from the first result\n",
    "        sigs = list(results[next(iter(results))].columns)\n",
    "\n",
    "    # Extract unique transcription factor (tf) names from all indices\n",
    "    tfs = list(set(tf for result in results.values() for tf in result.index))\n",
    "\n",
    "    # Initialize arrays for scores and counts\n",
    "    tf_scores = np.zeros((len(tfs), len(sigs)))\n",
    "    tf_counts = np.zeros((len(tfs), len(sigs)))\n",
    "\n",
    "    # Create a mapping from transcription factor names to indices\n",
    "    tf_index_map = {t: idx for idx, t in enumerate(tfs)}\n",
    "\n",
    "    # Aggregate scores and counts\n",
    "    for l, result in results.items():\n",
    "        for t in result.index:\n",
    "            temp = result.loc[t]\n",
    "            idx = tf_index_map[t]\n",
    "            if len(temp.shape) == 1:\n",
    "                tf_scores[idx, :] += temp.values\n",
    "                tf_counts[idx, :] += 1\n",
    "            else:\n",
    "                for ii in range(temp.shape[0]):\n",
    "                    tf_scores[idx, :] += temp.iloc[ii, :].values\n",
    "                    tf_counts[idx, :] += 1\n",
    "\n",
    "    # Calculate the mean scores by dividing tf_scores by tf_counts element-wise\n",
    "    mean_scores = pd.DataFrame(np.round(np.divide(tf_scores, tf_counts, out=np.zeros_like(tf_scores), where=tf_counts != 0)), index=tfs, columns=sigs)\n",
    "\n",
    "    return mean_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ./raw_data/chea3libs/ARCHS4_Coexpression.gmt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1628it [00:00, 35693.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ./raw_data/chea3libs/ReMap_ChIP-seq.gmt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "297it [00:00, 4728.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ./raw_data/chea3libs/Enrichr_Queries.gmt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1404it [00:00, 26203.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ./raw_data/chea3libs/GTEx_Coexpression.gmt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1607it [00:00, 21202.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ./raw_data/chea3libs/ENCODE_ChIP-seq.gmt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "552it [00:00, 7549.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ./raw_data/chea3libs/Literature_ChIP-seq.gmt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "307it [00:00, 5293.66it/s]\n",
      "100%|██████████| 172/172 [6:23:24<00:00, 133.75s/it]  \n"
     ]
    }
   ],
   "source": [
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "fisher = py.enrichment.FastFisher(34000)\n",
    "\n",
    "# Pre-read the GMT files and store the results in a dictionary\n",
    "libraries = {}\n",
    "for lib in os.listdir(f\"{raw_data}/chea3libs\"):\n",
    "    libraries[lib] = read_gmt_file(f\"{raw_data}/chea3libs/{lib}\", mode='raw')\n",
    "\n",
    "# Process each item in geo_split\n",
    "mranks = []\n",
    "\n",
    "for i in tqdm.tqdm(range(len(geo_split))):\n",
    "    results = {}\n",
    "    for lib_name, lib_gmt in libraries.items():\n",
    "        # Use the pre-read library data\n",
    "        res = py.enrichment.fisher(geo_split[i], lib_gmt, min_set_size=10, verbose=False, fisher=fisher)\n",
    "        temp = py.enrichment.consolidate(res).rank(axis=0)\n",
    "        temp.index = [x.split(\"_\")[0] for x in temp.index]\n",
    "        results[lib_name] = temp\n",
    "    \n",
    "    mr = mean_rank(results)\n",
    "    mranks.append(mr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the mean ranks matrix to a file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mranks = pd.concat(mranks, axis=1)\n",
    "mranks.to_csv(f\"{output}/mean_ranks_matrix.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary pruning\n",
    "Working from the mean ranks matrix, we perform two preliminary pruning steps:\n",
    "\n",
    "1. Remove all but the **top 10** most highly ranked TFs for each signature, which are most likely to be regulators of the genes in their respective signatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload TF rank matrix\n",
    "def preprocess(path):\n",
    "  matrix = pd.read_csv(path)\n",
    "  matrix.index = matrix.iloc[:, 0]\n",
    "  matrix = matrix.drop(columns = matrix.columns[0])\n",
    "  matrix = matrix.astype(np.int64)\n",
    "  return matrix\n",
    "\n",
    "# produces a library of signatures and the most highly enriched TFs (rank <= 10)\n",
    "def filter_by_rank(matrix, rank_method = \"min\", threshold = 15):\n",
    "\n",
    "  max_rank = 10 \n",
    "\n",
    "  results = {Signature : None for Signature in matrix.columns}\n",
    "\n",
    "  for Signature in matrix.columns:\n",
    "    # this is ranking the index based on the TF rankings, so then the index is used to access the TF name\n",
    "    rank_index = matrix[Signature].rank(method = rank_method).astype(int)\n",
    "    rank_top_tfs = [rank <= max_rank for rank in rank_index]\n",
    "\n",
    "    # Remove outliers above specified threshold\n",
    "    if len(matrix.index[rank_top_tfs]) <= threshold:\n",
    "      results[Signature] = matrix.index[rank_top_tfs].to_list()\n",
    "    else:\n",
    "      del results[Signature]\n",
    "  \n",
    "  # returns a matrix of gene set names x top 30 transcription factors (basically transposed GMT)\n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_ranks_matrix = preprocess(f\"{output}/mean_ranks_matrix.csv\")\n",
    "high_rank_matrix = filter_by_rank(mean_ranks_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Remove any GSE studies without clear **control and perturbation** groups, which ambiguates the sign of the gene set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29328\n"
     ]
    }
   ],
   "source": [
    "gses_to_keep_f = f\"{raw_data}/single_perturbation_gses.txt\"\n",
    "gses_to_keep = []\n",
    "filtered_matrix = {} # make a copy to be safe \n",
    "\n",
    "with open(gses_to_keep_f, 'r') as file:\n",
    "    gses_to_keep = [line.strip() for line in file]\n",
    "\n",
    "for sig in high_rank_matrix.keys():\n",
    "    gse_tag = sig.split(\"-\")[0].strip()\n",
    "    if gse_tag in gses_to_keep:\n",
    "        filtered_matrix[sig] = high_rank_matrix[sig]\n",
    "\n",
    "print(len(filtered_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save output as `.gmt` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29328/29328 [00:00<00:00, 687225.12it/s]\n"
     ]
    }
   ],
   "source": [
    "def write_gmt(path, library, transpose=False):\n",
    "  with open(path, \"w\") as file:\n",
    "    for term, items in tqdm.tqdm(library.items()):\n",
    "      file.write(f\"{term}\\t\\t\")\n",
    "      for item in items:\n",
    "        if transpose:\n",
    "          signature_join = \"-\".join(item.split())\n",
    "          file.write(f\"{signature_join}\\t\")\n",
    "        else:\n",
    "          file.write(f\"{item}\\t\")\n",
    "      file.write(\"\\n\")\n",
    "\n",
    "write_gmt(f\"{output}/filtered_tfsets.gmt\", filtered_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the TF-TF edge list \n",
    "### Count the number of times each TF-TF interaction occurs.\n",
    "* Create the transpose of the GMT  where each line is a TF followed by the gene sets in which it is ranked (`transpose_human.gmt`)\n",
    "* For a given highly ranked TF, find all sets where it is present. Mark a source-target edge with any TFs present in the same set.\\\n",
    "* **Directionality** of the edge is determined by the sign (up/down) of the gene set where the relationship occurs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the transpose matrix of a GMT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmt_transpose(library):\n",
    "  tfs = list(set([tf for sublist in library.values() for tf in sublist]))\n",
    "  transpose = {}\n",
    "  for tf in tqdm.tqdm(tfs):\n",
    "    collect = []\n",
    "    for signature in library:\n",
    "      if tf in library[signature]:\n",
    "        collect.append(signature)\n",
    "    transpose[tf] = set(collect)\n",
    "  return dict(transpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ./edge_constructing_files/filtered_tfsets.gmt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29328it [00:00, 242532.51it/s]\n",
      "100%|██████████| 1588/1588 [00:02<00:00, 656.06it/s]\n"
     ]
    }
   ],
   "source": [
    "tf_sets = read_gmt_file(f\"{output}/filtered_tfsets.gmt\", mode='signed')\n",
    "tf_transpose = gmt_transpose(tf_sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1588/1588 [00:00<00:00, 11181.06it/s]\n"
     ]
    }
   ],
   "source": [
    "write_gmt(f\"{output}/tf_transpose.gmt\", tf_transpose, transpose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store counts in a nested dictionary\\\n",
    "Dimensions: source_tf --> target_tf --> direction (up/down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading raw_data/human-geo-auto.gmt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "171441it [00:19, 8748.74it/s] \n"
     ]
    }
   ],
   "source": [
    "geo_gmt = read_gmt_file(\"raw_data/human-geo-auto.gmt\", mode='raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1588/1588 [00:36<00:00, 44.04it/s]\n"
     ]
    }
   ],
   "source": [
    "high_rank_tfs = list(tf_transpose.keys())\n",
    "\n",
    "# Create dict\n",
    "edge_matrix = {source : {target : {\n",
    "  \"+\": 0,\n",
    "  \"-\": 0\n",
    "} for target in high_rank_tfs} for source in high_rank_tfs}\n",
    "\n",
    "# Calculate counts\n",
    "for source in tqdm.tqdm(high_rank_tfs):\n",
    "  for signature in tf_transpose[source]:\n",
    "\n",
    "    spl = signature.rsplit(\" \", 1)\n",
    "    dir = \"+\" if spl[1] == \"up\" else \"-\" # \"dn\"\n",
    "    joined_sig = \" \".join(spl)\n",
    "\n",
    "    # if the gene set name is in the keys for the rummaGEO GMT\n",
    "    if joined_sig in geo_gmt.keys():\n",
    "      \n",
    "      # for each TF in the list of all TFs\n",
    "      for target in high_rank_tfs:\n",
    "        \n",
    "        if target in geo_gmt[joined_sig]: # TF is in list of DEGs\n",
    "          edge_matrix[source][target][dir] += 1\n",
    "   \n",
    "    else:\n",
    "      raise Exception(\"Signature {} not found\".format(joined_sig))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flatten results into pandas DataFrame object. Only save edges with count > 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1588/1588 [00:00<00:00, 2444.82it/s]\n"
     ]
    }
   ],
   "source": [
    "s = edge_matrix.keys()\n",
    "t = {target for targets in edge_matrix.values() for target in targets.keys()}\n",
    "targets = list(t)\n",
    "\n",
    "hindex = pd.MultiIndex.from_product([s, t, [\"+\", \"-\"]],\n",
    "  names = [\"source\", \"target\", \"direction\"])\n",
    "\n",
    "filtered_mat = []\n",
    "\n",
    "for source, targets in tqdm.tqdm(edge_matrix.items()):\n",
    "  for target, data in targets.items():\n",
    "    for direction, count in data.items():\n",
    "      if count >= 10:\n",
    "        filtered_mat.append((source, target, direction, count))\n",
    "\n",
    "df = pd.DataFrame(filtered_mat, columns =['source', 'target', 'direction', 'count'])\n",
    "df.set_index(['source', 'target', 'direction'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save results as a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f\"{output}/edge_list_unfiltered.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional - visualize count distributions\n",
    "### Generates three log-scale histograms of TF-TF interaction counts: all counts, negative counts, and positive counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot log-scaled histogram of counts\n",
    "\n",
    "def tf_histogram(name, counts, num_bins=300, fig_size=(10,6)):\n",
    "    plt.figure(figsize=fig_size)\n",
    "    plt.hist(counts, bins = num_bins, edgecolor ='none')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Interaction count', fontsize=14)\n",
    "    plt.ylabel('Frequency', fontsize=14)\n",
    "    if name == \"all\":\n",
    "        plt.title('All TF-TF interaction counts', fontsize=16)\n",
    "    elif name == \"pos\":\n",
    "        plt.title('Positive interaction counts', fontsize=16)\n",
    "    elif name == \"neg\": \n",
    "        plt.title('Negative interaction counts', fontsize=16)\n",
    "\n",
    "\n",
    "# get count values\n",
    "all_counts = df['count'].dropna()\n",
    "# Filter the DataFrame for rows where the direction is \"+\"\n",
    "positive_counts = df.loc[(slice(None), slice(None), \"+\"), :]['count']\n",
    "# Filter the DataFrame for rows where the direction is \"+\"\n",
    "negative_counts = (df.loc[(slice(None), slice(None), \"-\"), :])['count']\n",
    "\n",
    "tf_histogram(\"all\", all_counts)\n",
    "plt.savefig(f\"{output}/img/all_counts_histo.png\")\n",
    "plt.show()\n",
    "\n",
    "tf_histogram(\"pos\", positive_counts)\n",
    "plt.savefig(f\"{output}/img/pos_counts_histo.png\")\n",
    "plt.show()\n",
    "\n",
    "tf_histogram(\"neg\", negative_counts)\n",
    "plt.savefig(f\"{output}/img/neg_counts_histo.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
