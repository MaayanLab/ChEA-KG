{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking a TF-TF network against literature-reported TF-TF interactions\n",
    "Use this notebook to benchmark the filtered networks produced from `filter_assertions.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import tqdm\n",
    "import random\n",
    "import statistics \n",
    "from scipy import stats\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set a random seed to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(314)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a subdirectory to hold the libraries used for benchmarking and one to store the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib_dir = './raw_data/benchmarking_libraries'\n",
    "output_dir = './benchmarking_results'\n",
    "\n",
    "if not os.path.exists(lib_dir):\n",
    "    os.mkdir(lib_dir)\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to upload the benchmarking files as source-target pairs. This function finds all such pairs in a GMT-formatted benchmarking file, where each GMT term is a TF target. Only targets found in the set of high-rank TFs are included, since these are the targets with which we constructed the original network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat(gene):\n",
    "    if gene == \"BORCS8MEF2B\":\n",
    "        return \"BORCS8-MEF2B\"\n",
    "    if \"NKX\" in gene:\n",
    "        return gene[0:-1] + \"-\" + gene[-1]\n",
    "    if gene == \"C11ORF95\":\n",
    "        return \"C11orf95\"\n",
    "    else:\n",
    "        return gene\n",
    "    \n",
    "def find_library_edges(lib, tflib):\n",
    "\n",
    "    '''\n",
    "    creates a dictionary of source-target pairs in a GMT-formatted benchmarking file\n",
    "    each term is the source, targets are TFs in that term's set\n",
    "    input: a library and set of valid transcription factors\n",
    "    return: a dataframe of (source,target) edges and edge counts\n",
    "    '''\n",
    "\n",
    "    edges = {}\n",
    "\n",
    "    # for each line in that library file, \n",
    "    with open(lib, 'r') as f:\n",
    "        for line in f:\n",
    "            # extract the gmt term and gene set\n",
    "            items = line.strip().split(\"\\t\")\n",
    "            term = items[0]\n",
    "\n",
    "            # check the term/set formatting\n",
    "            if items[1] != \"\":\n",
    "                targets = items[1:]\n",
    "            else:\n",
    "                targets = items[2:]\n",
    "\n",
    "            # extract the source TF from the term name \n",
    "            parts = term.strip().split(\" \")\n",
    "\n",
    "            if len(parts) == 1:\n",
    "                source = term.split(\"_\")[0].upper()\n",
    "            else:\n",
    "                source = parts[0].upper()\n",
    "            \n",
    "            source = reformat(source)\n",
    "            # check if source is in transcript library -- if not, we know it's a TF, add it\n",
    "            if source not in tflib and source.upper() != \"C11ORF95\":\n",
    "                tflib.append(source)\n",
    "\n",
    "            # add the edge to the list\n",
    "            for target in targets:\n",
    "                t = reformat(target.upper())\n",
    "                if t in tflib and t.upper() != \"C11ORF95\":\n",
    "                    if (source,t) in edges.keys():\n",
    "                        edges[(source,t)] += 1\n",
    "                    else:\n",
    "                        edges[(source,t)] = 1\n",
    "\n",
    "    # form dataframe and add counts\n",
    "    edgeidx = pd.MultiIndex.from_tuples(edges.keys(), names = ['source', 'target'])\n",
    "    edgedf = pd.DataFrame(index = edgeidx, columns = ['count'])\n",
    "\n",
    "    for (s,t), count in edges.items():\n",
    "        edgedf.loc[(s,t)] = count\n",
    "    \n",
    "    return edgedf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function compares a specified network edge list against all three benchmarking libraries. The significance of the overlap between the two networks is determined by randomly shuffling the input network 100 times, then performing a z-test using the expected overlap and standard deviation. The results are saved to a CSV file for easy inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_edges(network_type, libraries):\n",
    "\n",
    "    '''\n",
    "    Compares an input network against a set of benchmarking libraries\n",
    "    input: the type of network being compared and the benchmarking libraries being used \n",
    "    return: a dataframe containing the results organized by benchmarking library\n",
    "    output: .csv file of the results for a single network\n",
    "    '''\n",
    "\n",
    "    # open network edge file and extract source,target tuples\n",
    "    network_edge_file = f'./filtered_edge_list/large/{network_type}/edge_list_filtered.csv'\n",
    "    network_edges = pd.read_csv(network_edge_file, usecols=[0, 1])\n",
    "    n_ntwrk_edges = len(network_edges)\n",
    "\n",
    "    # store results individually for each library \n",
    "    results = {}\n",
    "    trial_undirected = {}\n",
    "    trial_directed = {}\n",
    "    obs_overlap_undirected = {}\n",
    "    obs_overlap_directed = {}\n",
    "\n",
    "    # for each benchmarking library, generate random trials and store the results in the dictionaries above\n",
    "    for blib, library_counts in libraries.items():\n",
    "\n",
    "        # calculate the DIRECTED overlap ratio \n",
    "        library_edgelist = library_counts.index.to_frame(index=False)\n",
    "        common_edges = pd.merge(library_edgelist, network_edges, how='inner')\n",
    "\n",
    "        overlap = len(common_edges) / n_ntwrk_edges\n",
    "\n",
    "        # calculate the undirected overlap ratio\n",
    "        network_edges_undirected = pd.DataFrame([sorted(edge) for edge in network_edges.to_numpy()], columns = ['source', 'target'])\n",
    "        library_edges_undirected = pd.DataFrame([sorted(edge) for edge in library_edgelist.to_numpy()], columns = ['source', 'target'])\n",
    "        common_edges_undirected = pd.merge(library_edges_undirected, network_edges_undirected, how='inner')\n",
    "\n",
    "        undirected_overlap = len(common_edges_undirected) / n_ntwrk_edges\n",
    "\n",
    "        # generate a random set of edges using the original benchmark library and calculate the significance of the observed overlap\n",
    "        N_TRIALS = 100\n",
    "        trial_results = {\n",
    "            'directed': np.zeros(N_TRIALS),\n",
    "            'undirected': np.zeros(N_TRIALS)\n",
    "        }\n",
    "\n",
    "        ntwrk_source_labels = list(set(network_edges['source']))\n",
    "        ntwrk_target_labels = list(set(network_edges['target']))\n",
    "\n",
    "        for i in tqdm.tqdm(range(N_TRIALS)):\n",
    "            # generate a new network using the weighted node distribution from the library network\n",
    "            rand_sources = random.choices(ntwrk_source_labels, k=n_ntwrk_edges)\n",
    "            rand_targets = random.choices(ntwrk_target_labels, k=n_ntwrk_edges)\n",
    "\n",
    "            # calculate the overlap and save \n",
    "            random_network = pd.DataFrame(zip(rand_sources, rand_targets), columns = ['source', 'target'])\n",
    "            random_network_undirected = pd.DataFrame([sorted(edge) for edge in random_network.to_numpy()], columns = ['source', 'target'])\n",
    "            \n",
    "            common_edges_trial = pd.merge(library_edgelist, random_network, how='inner')\n",
    "            common_edges_undir_trial = pd.merge(library_edges_undirected, random_network_undirected, how='inner')\n",
    "\n",
    "            trial_results['directed'][i] = len(common_edges_trial) / n_ntwrk_edges\n",
    "            trial_results['undirected'][i] = len(common_edges_undir_trial) / n_ntwrk_edges\n",
    "        \n",
    "        # calculate statistics of random trials\n",
    "        expected_overlap = statistics.mean(trial_results['directed'])\n",
    "        overlap_stdev = statistics.stdev(trial_results['directed'])\n",
    "\n",
    "        expected_overlap_undir = statistics.mean(trial_results['undirected'])\n",
    "        overlap_stdev_undir = statistics.stdev(trial_results['undirected'])\n",
    "\n",
    "        z_score = (overlap - expected_overlap) / overlap_stdev\n",
    "        undir_z_score = (undirected_overlap - expected_overlap_undir) / overlap_stdev_undir\n",
    "        \n",
    "        # only calculate p-value for cases where there is more than expected overlap\n",
    "        p_value = stats.norm.sf(z_score)\n",
    "        undir_p = stats.norm.sf(undir_z_score)\n",
    "\n",
    "        benchmarklib = (blib.split(\"/\")[-1]).split(\".\")[0].replace(\"_\", \" \")\n",
    "\n",
    "        # save results\n",
    "        trial_undirected[benchmarklib] = trial_results['undirected']\n",
    "        trial_directed[benchmarklib] = trial_results['directed']\n",
    "        obs_overlap_directed[benchmarklib] = overlap\n",
    "        obs_overlap_undirected[benchmarklib] = undirected_overlap\n",
    "        \n",
    "        results[benchmarklib] = {\n",
    "            'library network size': len(library_counts),\n",
    "            'input network size': n_ntwrk_edges,\n",
    "            'observed overlap, directed': overlap,\n",
    "            'observed overlap, undirected': undirected_overlap,\n",
    "            'expected overlap, directed': expected_overlap,\n",
    "            'expected overlap, undirected': expected_overlap_undir,\n",
    "            'stdev, directed': overlap_stdev,\n",
    "            'stdev, undirected': overlap_stdev_undir,\n",
    "            'p-value directed': p_value,\n",
    "            'p-value, undirected': undir_p\n",
    "            }\n",
    "\n",
    "    undirected = dict(sorted(trial_undirected.items(), key=lambda item: statistics.mean(item[1])))\n",
    "    directed = dict(sorted(trial_directed.items(), key=lambda item: statistics.mean(item[1])))\n",
    "    resultsdf = pd.DataFrame(results)\n",
    "\n",
    "    resultsdf.to_csv(f\"{output_dir}/{network_type}.csv\")\n",
    "    \n",
    "    return {\n",
    "        'directed': directed, \n",
    "        'undirected': undirected, \n",
    "        'overlap-dir': obs_overlap_directed, \n",
    "        'overlap-undir': obs_overlap_undirected\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the full list of high-rank TFs using `mean_ranks_matrix.csv`  (produced by `construct_edge_list.ipynb`). This will be our library of TFs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load only the first column of the mean ranks matrix, which contains all of the TFs identified by ChEA3\n",
    "path_to_mean_ranks = './edge_constructing_files/mean_ranks_matrix.csv'\n",
    "tf_library = pd.read_csv(path_to_mean_ranks, usecols=[0])\n",
    "tf_library = tf_library.iloc[:, 0].tolist()\n",
    "tf_library = [reformat(tf.upper()) for tf in tf_library]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use three libraries to perform the benchmarking. 1. - 2. are Enrichr libraries, and 3. - 8. are the six ChEA3 primary libraries used in `construct_edge_list.ipynb`:\n",
    "1. TRANSFAC and JASPAR PWMs\n",
    "2. TRRUST Transcription Factors 2019\n",
    "3. ARCHS4 Coexpression\n",
    "4. ENCODE ChIP-seq\n",
    "5. Enrichr Queries\n",
    "6. Literature ChIP-seq\n",
    "7. ReMap ChIP-seq\n",
    "8. GTEx coexpression\n",
    "\n",
    "Download the Enrichr libraries by navigating here and clicking on their names: https://maayanlab.cloud/Enrichr/#libraries.\\\n",
    "Move all six libraries into `raw_data/benchmarking_libraries`, copying the six ChEA3 libraries from `raw_data/chea3libs`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the benchmarking library names from their file paths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify order of files to make processing easier \n",
    "enrichrfiles = glob.glob(f'{lib_dir}/*.txt')\n",
    "chea3files = glob.glob(f'{lib_dir}/*.gmt')\n",
    "\n",
    "allfiles = enrichrfiles + chea3files\n",
    "\n",
    "# format filenames\n",
    "filenames = [(file.split(\"/\")[-1]).split(\".\")[0].replace(\"_\", \" \") for file in allfiles]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the benchmarking results for all three network types. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "libraries = {file:find_library_edges(file, tf_library) for file in allfiles}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntypes = ['signature_shuffling', 'node_weighted']\n",
    "network_results = {ntype:benchmark_edges(ntype, libraries) for ntype in ntypes}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initate dict to store results - will be keyed by library\n",
    "undirected_reform = {lib:{'network':[],'values':[]} for lib in filenames}\n",
    "directed_reform = {lib:{'network':[],'values':[]} for lib in filenames}\n",
    "obsv_directed = {lib:{'network':[],'values':[]} for lib in filenames}\n",
    "obsv_undirected = {lib:{'network':[],'values':[]} for lib in filenames}\n",
    "\n",
    "ntrials = 100\n",
    "for ntwrk, libs in network_results.items():\n",
    "    for lib,value1 in libs['undirected'].items():\n",
    "        undirected_reform[lib]['network'].append(ntwrk)\n",
    "        undirected_reform[lib]['values'].append(value1.tolist())\n",
    "\n",
    "    for lib, value2 in libs['directed'].items():\n",
    "        directed_reform[lib]['network'].append(ntwrk)\n",
    "        directed_reform[lib]['values'].append(value2.tolist())\n",
    "\n",
    "    for lib,value3 in libs['overlap-dir'].items():\n",
    "        obsv_directed[lib]['network'].append(ntwrk)\n",
    "        obsv_directed[lib]['values'].append(value3)\n",
    "\n",
    "    for lib, value4 in libs['overlap-undir'].items():\n",
    "        obsv_undirected[lib]['network'].append(ntwrk)\n",
    "        obsv_undirected[lib]['values'].append(value4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lib in filenames:\n",
    "    obsv_dir = pd.DataFrame.from_dict(obsv_directed[lib]).set_index('network').reset_index()\n",
    "    obsv_undir = pd.DataFrame.from_dict(obsv_undirected[lib]).set_index('network').reset_index()\n",
    "    results = pd.DataFrame.from_dict(directed_reform[lib]).set_index('network')['values'].explode().reset_index()\n",
    "    results2 = pd.DataFrame.from_dict(undirected_reform[lib]).set_index('network')['values'].explode().reset_index()\n",
    "\n",
    "    for df in [obsv_dir, obsv_undir, results, results2]:\n",
    "        df.replace(to_replace={'signature_shuffling':'TSS-filtered GRN','node_weighted':'ND-filtered GRN'}, inplace=True)\n",
    "    f, ax = plt.subplots(nrows=2, ncols=1, figsize=(7,8), dpi=300)\n",
    "    print(lib)\n",
    "\n",
    "    plt.subplots_adjust(hspace=0.4)\n",
    "\n",
    "    sns.set_theme(font_scale=2)\n",
    "\n",
    "    sns.boxplot(results, ax = ax[0], x='values', y='network', hue='network', palette='vlag', width=0.5)\n",
    "    sns.stripplot(results, ax = ax[0], x='values', y='network', size=4, color=\".5\")\n",
    "    sns.stripplot(obsv_dir, ax = ax[0], x='values', y='network', size=8, color=\"black\")\n",
    "    ax[0].set_xlabel('Overlap, directed')\n",
    "    ax[0].set_ylabel(None)\n",
    "    ax[0].set_title(lib)\n",
    "\n",
    "    sns.set_theme(font_scale=2)\n",
    "    sns.boxplot(results2, ax = ax[1], x='values', y='network', hue='network', palette='vlag', width=0.5)\n",
    "    sns.stripplot(results2, ax = ax[1], x='values', y='network', size=4, color=\".5\")\n",
    "    sns.stripplot(obsv_undir, ax = ax[1], x='values', y='network', size=8, color=\"black\")\n",
    "    ax[1].set_xlabel(\"Overlap, undirected\")\n",
    "    ax[1].set_ylabel(None)\n",
    "\n",
    "    plt.savefig(f\"../figures/{lib}.svg\", format=\"svg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
