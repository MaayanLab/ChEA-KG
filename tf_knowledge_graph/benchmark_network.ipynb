{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking a TF-TF network against literature-reported TF-TF interactions\n",
    "Use this notebook to benchmark the filtered networks produced from `filter_assertions.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import tqdm\n",
    "import random\n",
    "import statistics \n",
    "from scipy import stats\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set a random seed to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(314)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a subdirectory to hold the libraries used for benchmarking and one to store the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib_dir = './raw_data/benchmarking_libraries'\n",
    "output_dir = './benchmarking_results'\n",
    "if not os.path.exists(lib_dir):\n",
    "    os.mkdir(lib_dir)\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to upload the benchmarking files as source-target pairs. This function finds all such pairs in a GMT-formatted benchmarking file, where each GMT term is a TF target. Only targets found in the set of high-rank TFs are included, since these are the targets with which we constructed the original network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_library_edges(lib, tflib):\n",
    "\n",
    "    '''\n",
    "    creates a dictionary of source-target pairs in a GMT-formatted benchmarking file\n",
    "    each term is the source, targets are TFs in that term's set\n",
    "    input: a library and set of valid transcription factors\n",
    "    return: a dataframe of (source,target) edges and edge counts\n",
    "    '''\n",
    "\n",
    "    edges = {}\n",
    "\n",
    "    # for each line in that library file, \n",
    "    with open(lib, 'r') as f:\n",
    "        for line in f:\n",
    "            # extract the gmt term and gene set\n",
    "            items = line.strip().split(\"\\t\")\n",
    "            term = items[0]\n",
    "\n",
    "            # check the term/set formatting\n",
    "            if items[1] != \"\":\n",
    "                targets = items[1:]\n",
    "            else:\n",
    "                targets = items[2:]\n",
    "\n",
    "            # extract the source TF from the term name \n",
    "            parts = term.strip().split(\" \")\n",
    "\n",
    "            if len(parts) == 1:\n",
    "                source = term.split(\"_\")[0].upper()\n",
    "            else:\n",
    "                source = parts[0].upper()\n",
    "            \n",
    "            # check if source is in transcript library -- if not, we know it's a TF, add it\n",
    "            if source not in tflib:\n",
    "                tflib.append(source)\n",
    "\n",
    "            # add the edge to the list\n",
    "            for target in targets:\n",
    "                target = target.upper()\n",
    "                if target in tflib:\n",
    "                    if (source,target) in edges.keys():\n",
    "                        edges[(source,target)] += 1\n",
    "                    else:\n",
    "                        edges[(source,target)] = 1\n",
    "\n",
    "    # form dataframe and add counts\n",
    "    edgeidx = pd.MultiIndex.from_tuples(edges.keys(), names = ['source', 'target'])\n",
    "    edgedf = pd.DataFrame(index = edgeidx, columns = ['count'])\n",
    "\n",
    "    for (s,t), count in edges.items():\n",
    "        edgedf.loc[(s,t)] = count\n",
    "    \n",
    "    return edgedf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function compares a specified network edge list against all three benchmarking libraries. The significance of the overlap between the two networks is determined by randomly shuffling the input network 100 times, then performing a z-test using the expected overlap and standard deviation. The results are saved to a CSV file for easy inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_edges(network_type, libraries):\n",
    "\n",
    "    '''\n",
    "    Compares an input network against a set of benchmarking libraries\n",
    "    input: the type of network being compared and the benchmarking libraries being used \n",
    "    return: a dataframe containing the results organized by benchmarking library\n",
    "    output: .csv file of the results for a single network\n",
    "    '''\n",
    "\n",
    "    # open network edge file and extract source,target tuples\n",
    "    network_edge_file = f'./filtered_edge_list/{network_type}/edge_list_filtered.csv'\n",
    "    network_edges = pd.read_csv(network_edge_file, usecols=[0, 1])\n",
    "    n_ntwrk_edges = len(network_edges)\n",
    "\n",
    "    results = {lib_name: None for lib_name in libraries}\n",
    "\n",
    "    # for each benchmarking library\n",
    "    for benchmarklib, library_counts in libraries.items():\n",
    "        library_edgelist = library_counts.index.to_frame(index=False)\n",
    "\n",
    "        # calculate the DIRECTED overlap ratio \n",
    "        # get unique source,target edges and group them by count \n",
    "        common_edges = pd.merge(library_edgelist, network_edges, how='inner')\n",
    "\n",
    "        # calculate the undirected overlap ratio\n",
    "        network_edges_undirected = pd.DataFrame([sorted(edge) for edge in network_edges.to_numpy()], columns = ['source', 'target'])\n",
    "        library_edges_undirected = pd.DataFrame([sorted(edge) for edge in library_edgelist.to_numpy()], columns = ['source', 'target'])\n",
    "        common_edges_undirected = pd.merge(library_edges_undirected, network_edges_undirected, how='inner')\n",
    "\n",
    "        overlap = len(common_edges) / n_ntwrk_edges\n",
    "        undirected_overlap = len(common_edges_undirected) / n_ntwrk_edges\n",
    "\n",
    "        # generate a random set of edges using the original benchmark library and calculate the significance of the observed overlap\n",
    "        N_TRIALS = 100\n",
    "        trial_results = {\n",
    "            'directed': np.zeros(N_TRIALS),\n",
    "            'undirected': np.zeros(N_TRIALS)\n",
    "        }\n",
    "\n",
    "        ntwrk_source_labels = list(set(network_edges['source']))\n",
    "        ntwrk_target_labels = list(set(network_edges['target']))\n",
    "\n",
    "        for i in tqdm.tqdm(range(N_TRIALS)):\n",
    "            # generate a new network using the weighted node distribution from the library network\n",
    "            rand_sources = random.choices(ntwrk_source_labels, k=n_ntwrk_edges)\n",
    "            rand_targets = random.choices(ntwrk_target_labels, k=n_ntwrk_edges)\n",
    "\n",
    "            # calculate the overlap and save \n",
    "            random_network = pd.DataFrame(zip(rand_sources, rand_targets), columns = ['source', 'target'])\n",
    "            random_network_undirected = pd.DataFrame([sorted(edge) for edge in random_network.to_numpy()], columns = ['source', 'target'])\n",
    "            \n",
    "            common_edges_trial = pd.merge(library_edgelist, random_network, how='inner')\n",
    "            common_edges_undir_trial = pd.merge(library_edges_undirected, random_network_undirected, how='inner')\n",
    "\n",
    "            trial_results['directed'][i] = len(common_edges_trial) / n_ntwrk_edges\n",
    "            trial_results['undirected'][i] = len(common_edges_undir_trial) / n_ntwrk_edges\n",
    "        \n",
    "\n",
    "        expected_overlap = statistics.mean(trial_results['directed'])\n",
    "        overlap_stdev = statistics.stdev(trial_results['directed'])\n",
    "\n",
    "        expected_overlap_undir = statistics.mean(trial_results['undirected'])\n",
    "        overlap_stdev_undir = statistics.stdev(trial_results['undirected'])\n",
    "\n",
    "        z_score = (overlap - expected_overlap) / overlap_stdev\n",
    "        undir_z_score = (undirected_overlap - expected_overlap_undir) / overlap_stdev_undir\n",
    "        \n",
    "        # only calculate p-value for cases where there is more than expected overlap\n",
    "        p_value = 1 - stats.norm.cdf(z_score)\n",
    "        undir_p = 1 - stats.norm.cdf(undir_z_score)\n",
    "\n",
    "        results[benchmarklib] = {\n",
    "            'library network size': len(library_counts),\n",
    "            'input network size': n_ntwrk_edges,\n",
    "            'observed overlap, directed': overlap,\n",
    "            'observed overlap, undirected': undirected_overlap,\n",
    "            'expected overlap, directed': expected_overlap,\n",
    "            'expected overlap, undirected': expected_overlap_undir,\n",
    "            'stdev, directed': overlap_stdev,\n",
    "            'stdev, undirected': overlap_stdev_undir,\n",
    "            'p-value directed': p_value,\n",
    "            'p-value, undirected': undir_p\n",
    "            }\n",
    "\n",
    "    resultsdf = pd.DataFrame(results)\n",
    "\n",
    "    resultsdf.to_csv(f\"{output_dir}/{network_type}.csv\")\n",
    "    return resultsdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the full list of high-rank TFs using `mean_ranks_matrix.csv`  (produced by `construct_edge_list.ipynb`). This will be our library of TFs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load only the first column of the mean ranks matrix, which contains all of the TFs identified by ChEA3\n",
    "path_to_mean_ranks = './edge_constructing_files/mean_ranks_matrix.csv'\n",
    "tf_library = pd.read_csv(path_to_mean_ranks, usecols=[0])\n",
    "tf_library = tf_library.iloc[:, 0].tolist()\n",
    "tf_library = [tf.upper() for tf in tf_library]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use three libraries to perform the benchmarking. 1. - 2. are Enrichr libraries, and 3. - 8. are the six ChEA3 primary libraries used in `construct_edge_list.ipynb`:\n",
    "1. TRANSFAC and JASPAR PWMs\n",
    "2. TRRUST Transcription Factors 2019\n",
    "3. ARCHS4 Coexpression\n",
    "4. ENCODE ChIP-seq\n",
    "5. Enrichr Queries\n",
    "6. Literature ChIP-seq\n",
    "7. ReMap ChIP-seq\n",
    "8. GTEx coexpression\n",
    "\n",
    "Download the Enrichr libraries by navigating here and clicking on their names: https://maayanlab.cloud/Enrichr/#libraries.\\\n",
    "Move all six libraries into `raw_data/benchmarking_libraries`, copying the six ChEA3 libraries from `raw_data/chea3libs`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the benchmarking library names from their file paths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify order of files to make processing easier \n",
    "enrichrfiles = glob.glob(f'{lib_dir}/*.txt')\n",
    "chea3files = glob.glob(f'{lib_dir}/*.gmt')\n",
    "\n",
    "allfiles = enrichrfiles + chea3files\n",
    "\n",
    "# format filenames\n",
    "filenames = [(file.split(\"/\")[-1]).split(\".\")[0].replace(\"_\", \" \") for file in allfiles]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the benchmarking results for all three network types. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "libraries = {file:find_library_edges(file, tf_library) for file in allfiles}\n",
    "ntypes = ['signature_shuffling', 'edge_weighted', 'node_weighted']\n",
    "network_results = [benchmark_edges(ntype, libraries) for ntype in ntypes]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
