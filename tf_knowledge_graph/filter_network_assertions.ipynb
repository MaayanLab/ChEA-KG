{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter TF network edges by expected counts\n",
    "Here we filter by significance the human TF interactions in the edge list produced by `construct_edge_list.ipynb`. Significance is determined by comparing the observed edge count to an average edge count produced by randomly shuffling the network. We test three shuffling methods: \n",
    "1. 'signature shuffling': this method randomly shuffles the target gene sets linked to each source, then produces a new edge count matrix\n",
    "2. 'node shuffling': this method reconstructs edges by randomly sampling a source node and target node from a weighted distribution, then drawing an edge between the two\n",
    "3. 'edge shuffling': this method builds an edge list by randomly sampling (source,target) edges from a weighted distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import statistics \n",
    "import scipy.stats as stats\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set a random seed to ensure reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(314)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set default directories\n",
    "- `input` should be the output directory from `construct_edge_list.ipynb`\n",
    "- `raw_data` should be the same raw data folder used in `construct_edge_list.ipynb`\n",
    "- `output` is built with three subdirectories to organize the assertions for benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = './edge_constructing_files'\n",
    "output = './filtered_edge_list'\n",
    "raw_data = './raw_data'\n",
    "\n",
    "if not os.path.exists(output): \n",
    "    os.mkdir(output)\n",
    "    os.mkdir(f'{output}/signature_shuffling') \n",
    "    os.mkdir(f'{output}/node_weighted')\n",
    "    os.mkdir(f'{output}/edge_weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload edge matrix to a dataframe and format it as a multiindex, where the index is a tuple of (source, target, direction). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_file = f\"{input}/edge_list_unfiltered.csv\"\n",
    "\n",
    "initial_counts = pd.read_csv(initial_file)\n",
    "\n",
    "initial_sources = initial_counts['source'].unique()\n",
    "initial_targets = initial_counts['target'].unique()\n",
    "\n",
    "initial_counts.set_index([\"source\", \"target\", \"direction\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate expected edge counts\n",
    "The general procedure for filtering expected counts is:\n",
    "1. Produce a randomly shuffled edge count matrix using one of the below methods\n",
    "2. Calculate the average and standard deviation of counts for each edge after N iterations\n",
    "3. Calculate the z-score and p-value of the observed edge counts compared to expected\n",
    "4. Remove any insignificant counts\n",
    "\n",
    "Three methods of generating expected counts are provided. To test all three methods, run the cells corresponding to one method, then proceed to \"Filter results by expected counts\". Repeat for all methods.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the number of trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRIALS = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Signature shuffling\n",
    "Shuffle signature sets between source TFs and recalculate counts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the benchmark method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffling_method = 'signature_shuffling'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload `tf_transpose.gmt` from `edge_constructing_files`. This creates a dictionary formatted as `tf:[signature1 ... signatureN]`, where each signature is a gene set enriched for that TF.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signature_file = f\"{input}/tf_transpose.gmt\"\n",
    "signature_sets = defaultdict(set)\n",
    "\n",
    "with open(signature_file, \"r\") as file:\n",
    "    for line in tqdm.tqdm(file):\n",
    "        tf, *signature = line.strip().split()\n",
    "        signature_sets[tf] = set(signature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload RummaGEO gene sets -- we need this again to be able to reproduce the edge count matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_gmt(path):\n",
    "  gmt = {}\n",
    "  print(\"Reading {}\".format(path))\n",
    "  with open(path, \"r\") as file:\n",
    "    for line in tqdm.tqdm(file):\n",
    "      # 'id' refers to up or down tag\n",
    "      signature, id, *tf = line.strip().split()\n",
    "      gmt[\" \".join([signature, id])] = set(tf)\n",
    "\n",
    "  return gmt\n",
    "\n",
    "geo_gmt = read_gmt(f\"{raw_data}/human-geo-auto.gmt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recalculate counts:\\\n",
    "First, shuffle the signature sets between TFs.\\\n",
    "Next, count all edges using the same edge-counting procedure as in construct_edge_list.ipynb. Since we repeat this multiple time, each edge count is actually an array of counts (one count for each iteration):\n",
    "```\n",
    "{ source: { \n",
    "    target: { \n",
    "            '+': np.array(up_counts_all_trials)\n",
    "            '-': np.array(dn_counts_all_trials)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_signatures(signature_sets):\n",
    "    tfs = list(signature_sets.keys())\n",
    "    sigs = list(signature_sets.values())\n",
    "    random.shuffle(sigs)\n",
    "    shuffled_sigsets = dict(zip(tfs, sigs))\n",
    "    return shuffled_sigsets\n",
    "\n",
    "all_high_tfs = list(signature_sets.keys())\n",
    "\n",
    "expected_counts = {source : {target : {\n",
    "    \"+\": np.zeros(N_TRIALS),\n",
    "    \"-\": np.zeros(N_TRIALS),\n",
    "  } for target in all_high_tfs} for source in all_high_tfs}\n",
    "\n",
    "for i in tqdm.tqdm(range(N_TRIALS)):\n",
    "  shuffled_sets = shuffle_signatures(signature_sets)\n",
    "\n",
    "  # Calculate counts\n",
    "  for source in all_high_tfs:\n",
    "    for signature in shuffled_sets[source]:\n",
    "\n",
    "      spl = signature.rsplit(\"-\", 1)\n",
    "      dir = \"+\" if spl[1] == \"up\" else \"-\" # \"dn\"\n",
    "      joined_sig = \" \".join(spl)\n",
    "\n",
    "      # if the gene set name is in the keys for the rummaGEO GMT\n",
    "      if joined_sig in geo_gmt.keys():\n",
    "        for target in all_high_tfs:\n",
    "          if target in geo_gmt[joined_sig]: \n",
    "            expected_counts[source][target][dir][i] += 1\n",
    "    \n",
    "      else:\n",
    "        raise Exception(\"Signature {} not found\".format(joined_sig))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Node weighted\n",
    "Randomly generate new edges by randomly selecting (weighted) source and target nodes independently   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the benchmark method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffling_method = 'node_weighted'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the weighted distribution for source and target nodes by calculating the proportion of all edges that involve a unique node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_counts = initial_counts.groupby(['source']).sum()\n",
    "target_counts = initial_counts.groupby(['target']).sum()\n",
    "\n",
    "source_weights = source_counts['count'].tolist()\n",
    "target_weights = target_counts['count'].tolist()\n",
    "\n",
    "num_edges = initial_counts['count'].sum()\n",
    "\n",
    "source_labels = source_counts.index.tolist()\n",
    "target_labels = target_counts.index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the expected counts by sampling a set of N random source and target nodes and drawing an edge between each pair, where N = the number of edges in the input edge list. Count the number of times each (source,target) edge occurs. Finally, convert the edge count matrix to a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_counts = {source : {target : {\n",
    "    \"+\": np.zeros(N_TRIALS),\n",
    "    \"-\": np.zeros(N_TRIALS),\n",
    "  } for target in initial_targets} for source in initial_sources}\n",
    "\n",
    "for i in tqdm.tqdm(range(N_TRIALS)):\n",
    "        # Perform weighted random choice\n",
    "        random_source = random.choices(source_labels, weights=source_weights, k=num_edges)\n",
    "        random_target = random.choices(target_labels, weights=target_weights, k=num_edges)\n",
    "          \n",
    "        for source, target in zip(random_source, random_target):\n",
    "                # randomly choose an edge direction\n",
    "                direction = random.randint(0,1)\n",
    "                if direction == 0:\n",
    "                        expected_counts[source][target][\"+\"][i] += 1\n",
    "                else:\n",
    "                        expected_counts[source][target][\"-\"][i] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3: Edge weighted\n",
    "Instead of sampling source and target nodes independently, produce a weighted distribution of edges based on their initial counts. Randomly sample N edges, where N is the size of the original edge matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the benchmark method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffling_method = 'edge_weighted'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Produce a weighted distribution of edges using their counts in the input edge list. Randomly sample N edges from this distribution, where N is the size of the input edge list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly select N edges from a network using weighted random sampling\n",
    "# N is the total number of edge counts in the input network\n",
    "def find_random_edges(network):\n",
    "        edge_counts = network.groupby(['source', 'target']).sum()\n",
    "        edge_weights = edge_counts['count'].tolist()\n",
    "        num_edges = sum(edge_weights)\n",
    "        edge_labels = edge_counts.index.tolist()\n",
    "        random_edges = random.choices(edge_labels, weights=edge_weights, k=num_edges)\n",
    "\n",
    "        return random_edges\n",
    "\n",
    "# initialize expected counts\n",
    "expected_counts = {source : {target : {\n",
    "    \"+\": np.zeros(N_TRIALS),\n",
    "    \"-\": np.zeros(N_TRIALS),\n",
    "  } for target in initial_targets} for source in initial_sources}\n",
    "\n",
    "for i in tqdm.tqdm(range(N_TRIALS)):\n",
    "        # Perform weighted random choice\n",
    "        random_edges = find_random_edges(initial_counts)     \n",
    "\n",
    "        for index in random_edges:\n",
    "                # extract the source, target and randomly assign a direction\n",
    "                source, target = index\n",
    "\n",
    "                # randomly choose an edge direction\n",
    "                direction = random.randint(0,1)\n",
    "\n",
    "                if direction == 0:\n",
    "                        expected_counts[source][target][\"+\"][i] += 1\n",
    "                else:\n",
    "                        expected_counts[source][target][\"-\"][i] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter results using expected counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, calculate the edge statistics using the expected counts - observed counts, expected counts, mean, stdev, z-score, and p-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a similar array to store stats\n",
    "hindex = pd.MultiIndex.from_product([initial_sources, initial_targets,['+', '-']],\n",
    "  names = [\"source\", \"target\", \"relation\"])\n",
    "edge_statistics = pd.DataFrame(index = hindex, columns = [\"observed\", \"expected\", \"expected stdev\", \"z-score\", \"p-value\"])\n",
    "\n",
    "for source in tqdm.tqdm(initial_sources):\n",
    "  for target in initial_targets:\n",
    "    for dir in ['+','-']:\n",
    "      trial_counts = expected_counts[source][target][dir]\n",
    "\n",
    "      # find expected and observed counts -- if that edge doesn't exist, then the observed counts are zero\n",
    "      try:\n",
    "        obsv_counts = initial_counts.loc[(source, target, dir), 'count']\n",
    "      except KeyError:\n",
    "        obsv_counts = 0\n",
    "\n",
    "      # calculate statistics \n",
    "      mean = statistics.mean(trial_counts)\n",
    "      stdev = statistics.stdev(trial_counts)\n",
    "\n",
    "      # ignore values with no stdev and expected counts == 0\n",
    "      if mean > 0 and stdev > 0:\n",
    "        z_score = (obsv_counts - mean) / stdev\n",
    "        p_value = 1 - stats.norm.cdf(z_score)\n",
    "\n",
    "        # store in dataframe, removing non-existent edges\n",
    "        if obsv_counts > 0:\n",
    "          edge_statistics.loc[(source, target, dir)] = [obsv_counts, mean, stdev, z_score, p_value]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove insignificant edges.\\\n",
    "*Z-score is the chosen method because it allows for finer filtering. Uncomment p-vaue as desired.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop NAs\n",
    "edge_statistics.dropna(inplace=True)\n",
    "\n",
    "p_sorted = edge_statistics.sort_values(by='p-value', ascending = True)\n",
    "P = 0.01\n",
    "significant_edges = edge_statistics.loc[edge_statistics['p-value'] < P]\n",
    "\n",
    "edges = significant_edges.index.to_frame(index=False)\n",
    "print(f\"Network type is {shuffling_method}\")\n",
    "print(\"Number of edges is \", len(edges))\n",
    "\n",
    "targets = edges['target'].value_counts()\n",
    "sources = edges['source'].value_counts()\n",
    "allnodes = targets.index.to_list() | sources.index.to_list()\n",
    "print(f\"Number of nodes is: {len(allnodes)}\")\n",
    "\n",
    "avg_in = statistics.mean(edges['target'].value_counts())\n",
    "avg_out = statistics.mean(edges['source'].value_counts())\n",
    "print(\"The average target degree is: \", avg_in)\n",
    "print(\"The average source degree is: \", avg_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For pairs that have significant edges in both directions, keep only the edge with the most significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# work from a copy\n",
    "significant_edges_copy = significant_edges.copy()\n",
    "edges_to_drop = []\n",
    "\n",
    "# search source-target pairs and retain only the edge with the highest significance \n",
    "for (source, target), group in significant_edges_copy.groupby(level=['source', 'target']):\n",
    "\n",
    "    directions = group.index.get_level_values('relation')\n",
    "\n",
    "    # remove either up or down if both are significant\n",
    "    if '+' in directions and '-' in directions:\n",
    "        # Filter to get '+' and '-' entries\n",
    "        up_data = group.loc[(slice(None), slice(None), '+'), :]\n",
    "        dn_data = group.loc[(slice(None), slice(None), '-'), :]   \n",
    "\n",
    "        # remove lower z-score\n",
    "        if up_data['z-score'].values[0] > dn_data['z-score'].values[0]:\n",
    "          edges_to_drop.append((source, target, '+')) \n",
    "        else:\n",
    "          edges_to_drop.append((source, target, '-'))\n",
    "\n",
    "significant_edges_copy.drop(edges_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a sense of the size of our network after filtering, let's look at the edge count as well as the distribution of node degrees for source and for target nodes. For more detailed network statistics, run `network_stats.py` on the KG-formatted network assertions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = significant_edges_copy.index.to_frame(index=False)\n",
    "print(f\"Network type is {shuffling_method}\")\n",
    "print(\"Number of edges is \", len(edges))\n",
    "\n",
    "targets = edges['target'].value_counts()\n",
    "sources = edges['source'].value_counts()\n",
    "allnodes = targets.index.to_list() | sources.index.to_list()\n",
    "print(f\"Number of nodes is: {len(allnodes)}\")\n",
    "\n",
    "avg_in = statistics.mean(edges['target'].value_counts())\n",
    "avg_out = statistics.mean(edges['source'].value_counts())\n",
    "print(\"The average target degree is: \", avg_in)\n",
    "print(\"The average source degree is: \", avg_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For demo network only:** \n",
    "In order to prevent the final network from being too dense to visualize, limit the maximum number of outgoing ('source') edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only the top 3 most significant edges from each source \\n\",\n",
    "max_src_edges = 3\n",
    "significant_edges_copy.sort_values(by=['source','z-score'], inplace=True)\n",
    "final_edge_list = significant_edges_copy.groupby(level='source').head(max_src_edges).sort_values(by=['source','z-score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_edge_list = significant_edges_copy.\n",
    "p_sorted.to_csv(f\"{output}/test/{shuffling_method}/p_sorted_edge_stats.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save edge list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_edge_list.to_csv(f\"{output}/test/{shuffling_method}/edge_list_filtered.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic network statistics including number of unique nodes and edges by type, average links per node, and number of self loops and feedback loops are defined in the module `network_statistics.py`. Ideally there are only a few thousand edges, with only a few average links per node. These criteria will make visualizing the whole network much easier, but are not required.\n",
    "\n",
    "Additionally, plot the connectivity distribution of the network nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import network_statistics as nstats\n",
    "\n",
    "outfile = f\"{output}/{shuffling_method}/edge_list_filtered.csv\"\n",
    "nstats.all_network_stats(outfile)\n",
    "nstats.plot_connectivity_distrib(outfile, f'{output}/{shuffling_method}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
