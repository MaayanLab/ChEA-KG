{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking a TF-TF network against literature-reported TF-TF interactions\n",
    "### Use this notebook to benchmark the filtered network produced from filter_assertions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate an edge list from benchmark libraries\n",
    "Three Enrichr libraries https://maayanlab.cloud/Enrichr/#libraries) are used to perform benchmarking\n",
    "1. TRANSFAC and JASPAR PWMs\n",
    "2. TRRUST Transcription Factors 2019\n",
    "3. Rummagene Transcription Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import tqdm\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics as stats\n",
    "import scipy.stats as spstats\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, load the full list of transcription factors used in the initial transcription factor ranking (build_assertions.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load only the first column of the mean ranks matrix, which contains all of the TFs identified by ChEA3 -- this will be our TF library\n",
    "path_to_mean_ranks = '/Users/anna/Projects/KG_UI/build_TF_network/build_network_out/mean_ranks_matrix.csv'\n",
    "tf_library = pd.read_csv(path_to_mean_ranks, usecols=[0])\n",
    "tf_library = tf_library.iloc[:, 0].tolist()\n",
    "tf_library = [tf.upper() for tf in tf_library]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, upload the gmt files as source-target pairs. Only include targets that are in the ChEA3 library list.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_dir = './raw_data/benchmarking_libraries'\n",
    "\n",
    "# specify order of files to make processing easier \n",
    "files = glob.glob(f'{benchmark_dir}/*.txt')\n",
    "\n",
    "# format filenames\n",
    "filenames = [(file.split(\"/\")[-1]).split(\".\")[0].replace(\"_\", \" \") for file in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_library_edges(lib):\n",
    "    edges = {}\n",
    "\n",
    "    # for each line in that library file, \n",
    "    with open(lib, 'r') as f:\n",
    "        for line in f:\n",
    "            # extract the gmt term and gene set\n",
    "            items = line.strip().split(\"\\t\")\n",
    "            term = items[0]\n",
    "            targets = items[2:]\n",
    "\n",
    "            # extract the source TF from the term name \n",
    "            libname = lib.split(\"/\")[-1]\n",
    "\n",
    "            if libname == 'Rummagene_transcription_factors.txt':\n",
    "                source = term.split(\" \")[-1].upper()\n",
    "            else:\n",
    "                source = term.split(\" \")[0].upper()\n",
    "            \n",
    "            # check if source is in transcript library -- if not, we know it's a TF, add it\n",
    "            if source not in tf_library:\n",
    "                tf_library.append(source)\n",
    "\n",
    "            # add the edge to the list -- will remove duplicates later\n",
    "            for target in targets:\n",
    "                target = target.upper()\n",
    "                if target in tf_library:\n",
    "                    if (source,target) in edges.keys():\n",
    "                        edges[(source,target)] += 1\n",
    "                    else:\n",
    "                        edges[(source,target)] = 1\n",
    "\n",
    "    # form dataframe and add counts\n",
    "    edgeidx = pd.MultiIndex.from_tuples(edges.keys(), names = ['source', 'target'])\n",
    "    edgedf = pd.DataFrame(index = edgeidx, columns = ['count'])\n",
    "\n",
    "    for (s,t), count in edges.items():\n",
    "        edgedf.loc[(s,t)] = count\n",
    "    \n",
    "    return edgedf\n",
    "\n",
    "# \n",
    "def benchmark_edges(network_type):\n",
    "    # open netwoork edge file and extract source,target tuples\n",
    "    network_edge_file = f'/Users/anna/Projects/KG_UI/build_TF_network/filter_assertions_out/benchmarking/{network_type}/edge_list_filtered.csv'\n",
    "    network_edges = pd.read_csv(network_edge_file, usecols=[0, 1])\n",
    "    n_ntwrk_edges = len(network_edges)\n",
    "\n",
    "    results = {file: None for file in files}\n",
    "\n",
    "    # for each benchmarking library\n",
    "    for benchmarklib in results.keys():\n",
    "        # find the edges in the library \n",
    "        library_counts = find_library_edges(benchmarklib)\n",
    "\n",
    "        library_edgelist = library_counts.index.to_frame(index=False)\n",
    "\n",
    "        # calculate the DIRECTED overlap ratio \n",
    "        # get unique source,target edges and group them by count \n",
    "        common_edges = pd.merge(library_edgelist, network_edges, how='inner')\n",
    "\n",
    "        # calculate the undirected overlap ratio\n",
    "        network_edges_undirected = pd.DataFrame([sorted(edge) for edge in network_edges.to_numpy()], columns = ['source', 'target'])\n",
    "        library_edges_undirected = pd.DataFrame([sorted(edge) for edge in library_edgelist.to_numpy()], columns = ['source', 'target'])\n",
    "        common_edges_undirected = pd.merge(library_edges_undirected, network_edges_undirected, how='inner')\n",
    "\n",
    "        overlap = len(common_edges)/n_ntwrk_edges\n",
    "        undirected_overlap = len(common_edges_undirected)/n_ntwrk_edges\n",
    "\n",
    "        # generate a random set of edges using the original benchmark library and calculate the significance of the observed overlap\n",
    "        N_TRIALS = 100\n",
    "        trial_results = {\n",
    "            'directed': np.zeros(N_TRIALS),\n",
    "            'undirected': np.zeros(N_TRIALS)\n",
    "        }\n",
    "\n",
    "        lib_source_counts = library_counts.groupby(['source']).sum()\n",
    "        lib_target_counts = library_counts.groupby(['target']).sum()\n",
    "\n",
    "        lib_source_weights = lib_source_counts['count'].tolist()\n",
    "        lib_target_weights = lib_target_counts['count'].tolist()\n",
    "\n",
    "        num_s_edges = sum(lib_source_weights)\n",
    "        num_t_edges = sum(lib_target_weights)\n",
    "\n",
    "        ntwrk_source_labels = list(set(network_edges['source']))\n",
    "        ntwrk_target_labels = list(set(network_edges['target']))\n",
    "\n",
    "        for i in tqdm.tqdm(range(N_TRIALS)):\n",
    "            # generate a new network using the weighted node distribution from the library network\n",
    "            rand_sources = random.choices(ntwrk_source_labels, k=n_ntwrk_edges)\n",
    "            rand_targets = random.choices(ntwrk_target_labels, k=n_ntwrk_edges)\n",
    "\n",
    "            # calculate the overlap and save \n",
    "            random_network = pd.DataFrame(zip(rand_sources, rand_targets), columns = ['source', 'target'])\n",
    "            random_network_undirected = pd.DataFrame([sorted(edge) for edge in random_network.to_numpy()], columns = ['source', 'target'])\n",
    "            \n",
    "            common_edges_trial = pd.merge(library_edgelist, random_network, how='inner')\n",
    "            common_edges_undir_trial = pd.merge(library_edges_undirected, random_network_undirected, how='inner')\n",
    "\n",
    "            trial_results['directed'][i] = len(common_edges_trial)/n_ntwrk_edges\n",
    "            trial_results['undirected'][i] = len(common_edges_undir_trial)/n_ntwrk_edges\n",
    "        \n",
    "        expected_overlap = stats.mean(trial_results['directed'])\n",
    "        overlap_stdev = stats.stdev(trial_results['directed'])\n",
    "\n",
    "        expected_overlap_undir = stats.mean(trial_results['undirected'])\n",
    "        overlap_stdev_undir = stats.stdev(trial_results['undirected'])\n",
    "\n",
    "        z_score = (overlap - expected_overlap) / overlap_stdev\n",
    "        undir_z_score = (undirected_overlap - expected_overlap_undir) / overlap_stdev_undir\n",
    "        \n",
    "        # only calculate p-value for cases where there is more than expected overlap\n",
    "        if overlap > expected_overlap:\n",
    "            p_value = 1 - spstats.norm.cdf(z_score)\n",
    "        else:\n",
    "            p_value = None\n",
    "        if undirected_overlap > expected_overlap_undir:\n",
    "            undir_p = 1 - spstats.norm.cdf(undir_z_score)\n",
    "        else:\n",
    "            undir_p = None\n",
    "\n",
    "        results[benchmarklib] = {\n",
    "            'library network size': len(library_counts),\n",
    "            'input network size': n_ntwrk_edges,\n",
    "            'observed overlap, directed': overlap * n_ntwrk_edges,\n",
    "            'observed overlap, undirected': undirected_overlap * n_ntwrk_edges,\n",
    "            'expected overlap, directed': expected_overlap * n_ntwrk_edges,\n",
    "            'expected overlap, undirected': expected_overlap_undir * n_ntwrk_edges,\n",
    "            'stdev, directed': overlap_stdev,\n",
    "            'stdev, undirected': overlap_stdev_undir,\n",
    "            'p-value directed': p_value,\n",
    "            'p-value, undirected': undir_p\n",
    "            }\n",
    "\n",
    "    resultsdf = pd.DataFrame(results)\n",
    "\n",
    "    resultsdf.to_csv(f\"./benchmarking_results/{network_type}.csv\")\n",
    "    return resultsdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate directed and undirected benchmarking results\n",
    "ntypes = ['signature_shuffling', 'edge_weighted', 'node_weighted']\n",
    "network_results = [benchmark_edges(ntype) for ntype in ntypes]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
