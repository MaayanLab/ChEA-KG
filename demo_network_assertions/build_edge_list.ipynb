{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an edge list for a TF-TF network using RummaGEO gene sets \n",
    "### This notebook explains how to produce an edge list of interactions between human TFs for ingestion into the Knowledge Graph UI (https://github.com/MaayanLab/Gene-Knowledge-Graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyenrichr as py\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import warnings\n",
    "import json\n",
    "import csv\n",
    "import statistics as stats\n",
    "import plotly.graph_objects as go\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = './raw_data'\n",
    "output = './build_edge_list_out'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF enrichment analysis of RummaGEO sets\n",
    "### For each gene set, produce a ranked list of all of 1,632 ChEA3 transcription factors\n",
    "TF enrichment analysis for each of the 171k RummaGEO gene sets is performed using a local version of ChEA3.\n",
    "\n",
    "*For more information:* \n",
    "> RummaGEO: https://rummageo.com/  \\\n",
    "> ChEA3: https://maayanlab.cloud/chea3/\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# read a GMT file and save to a dictionary\n",
    "\n",
    "def read_gmt1(gmt_file_path):\n",
    "    gene_sets = {}\n",
    "    \n",
    "    with open(gmt_file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) > 5:\n",
    "                gene_sets[parts[0].strip()] = set(parts[1:])\n",
    "    return gene_sets\n",
    "\n",
    "\n",
    "# divide a dictionary into N subdictionaries\n",
    "\n",
    "def split_dict(original_dict, N):\n",
    "    iter_dict = iter(original_dict.items())\n",
    "    list_of_dicts = []\n",
    "    \n",
    "    while True:\n",
    "        small_dict = {}\n",
    "        \n",
    "        try:\n",
    "            for _ in range(N):\n",
    "                key, value = next(iter_dict)\n",
    "                small_dict[key] = value\n",
    "        except StopIteration:\n",
    "            if small_dict:\n",
    "                list_of_dicts.append(small_dict)\n",
    "            break\n",
    "\n",
    "        list_of_dicts.append(small_dict)\n",
    "    \n",
    "    return list_of_dicts\n",
    "\n",
    "\n",
    "# determine the mean rank of a TF across all ChEA3 libraries\n",
    "\n",
    "def mean_rank(results):\n",
    "    if results:\n",
    "        # Extract gene set names from the first result\n",
    "        sigs = list(results[next(iter(results))].columns)\n",
    "\n",
    "    # Extract unique transcription factor (tf) names from all indices\n",
    "    tfs = list(set(tf for result in results.values() for tf in result.index))\n",
    "\n",
    "    # Initialize arrays for scores and counts\n",
    "    tf_scores = np.zeros((len(tfs), len(sigs)))\n",
    "    tf_counts = np.zeros((len(tfs), len(sigs)))\n",
    "\n",
    "    # Create a mapping from transcription factor names to indices\n",
    "    tf_index_map = {t: idx for idx, t in enumerate(tfs)}\n",
    "\n",
    "    # Aggregate scores and counts\n",
    "    for l, result in results.items():\n",
    "        for t in result.index:\n",
    "            temp = result.loc[t]\n",
    "            idx = tf_index_map[t]\n",
    "            if len(temp.shape) == 1:\n",
    "                tf_scores[idx, :] += temp.values\n",
    "                tf_counts[idx, :] += 1\n",
    "            else:\n",
    "                for ii in range(temp.shape[0]):\n",
    "                    tf_scores[idx, :] += temp.iloc[ii, :].values\n",
    "                    tf_counts[idx, :] += 1\n",
    "\n",
    "    # Calculate the mean scores by dividing tf_scores by tf_counts element-wise\n",
    "    mean_scores = pd.DataFrame(np.round(np.divide(tf_scores, tf_counts, out=np.zeros_like(tf_scores), where=tf_counts != 0)), index=tfs, columns=sigs)\n",
    "\n",
    "    return mean_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import RummaGEO data and split into 1k subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rummageo = f\"{raw_data}/human-geo-auto.gmt\"\n",
    "geo = read_gmt1(rummageo)\n",
    "geo_split = split_dict(geo, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each gene set, rank TFs based on enrichment and record the mean rank across ChEA3 libraries.\\\n",
    "**This step can take several hours.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "fisher = py.enrichment.FastFisher(34000)\n",
    "\n",
    "# Pre-read the GMT files and store the results in a dictionary\n",
    "libraries = {}\n",
    "for lib in os.listdir(\"raw_data/chea3libs\"):\n",
    "    libraries[lib] = read_gmt1(f\"raw_data/chea3libs/{lib}\")\n",
    "\n",
    "# Process each item in geo_split\n",
    "mranks = []\n",
    "\n",
    "for i in tqdm.tqdm(range(len(geo_split))):\n",
    "    results = {}\n",
    "    for lib_name, lib_gmt in libraries.items():\n",
    "        # Use the pre-read library data\n",
    "        res = py.enrichment.fisher(geo_split[i], lib_gmt, min_set_size=10, verbose=False, fisher=fisher)\n",
    "        temp = py.enrichment.consolidate(res).rank(axis=0)\n",
    "        temp.index = [x.split(\"_\")[0] for x in temp.index]\n",
    "        results[lib_name] = temp\n",
    "    \n",
    "    mr = mean_rank(results)\n",
    "    mranks.append(mr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save mean ranks matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mranks = pd.concat(mranks, axis=1)\n",
    "mranks.to_csv(f\"{output}/mean_ranks_matrix.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary filtering\n",
    "* Retain only the **top 10** highly ranked TFs for each set\n",
    "* Retain only GSE studies with clear **control and perturbation** groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload TF rank matrix\n",
    "\n",
    "def preprocess(path):\n",
    "  matrix = pd.read_csv(path)\n",
    "  matrix.index = matrix.iloc[:, 0]\n",
    "  matrix = matrix.drop(columns = matrix.columns[0])\n",
    "  matrix = matrix.astype(np.int64)\n",
    "  return matrix\n",
    "\n",
    "\n",
    "# produce a library of signatures and the most highly enriched TFs (rank <= 10)\n",
    "\n",
    "def filter_by_rank(matrix, rank_method = \"min\", threshold = 15):\n",
    "\n",
    "  max_rank = 10 \n",
    "\n",
    "  results = {Signature : None for Signature in matrix.columns}\n",
    "\n",
    "  for Signature in matrix.columns:\n",
    "    # this is ranking the index based on the TF rankings, so then the index is used to access the TF name\n",
    "    rank_index = matrix[Signature].rank(method = rank_method).astype(int)\n",
    "    rank_top_tfs = [rank <= max_rank for rank in rank_index]\n",
    "\n",
    "    # Remove outliers above specified threshold\n",
    "    if len(matrix.index[rank_top_tfs]) <= threshold:\n",
    "      results[Signature] = matrix.index[rank_top_tfs].to_list()\n",
    "    else:\n",
    "      del results[Signature]\n",
    "  \n",
    "  # returns a matrix of gene set names x top 30 transcription factors (basically transposed GMT)\n",
    "  return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_ranks_matrix = preprocess(f\"{output}/mean_ranks_matrix.csv\")\n",
    "high_rank_matrix = filter_by_rank(mean_ranks_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now filter out GSE studies that do not clearly include control and perturbation groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_keep = f\"{raw_data}/single_perturbation_gses.txt\"\n",
    "gses_to_keep = []\n",
    "filtered_matrix = {} # make a copy to be safe \n",
    "\n",
    "with open(file_to_keep, 'r') as file:\n",
    "    gses_to_keep = [line.strip() for line in file]\n",
    "\n",
    "for sig in high_rank_matrix.keys():\n",
    "    gse_tag = sig.split(\"-\")[0]\n",
    "    if gse_tag in gses_to_keep:\n",
    "        filtered_matrix[sig] = high_rank_matrix[sig]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save output as `.gmt` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29330/29330 [00:00<00:00, 427097.18it/s]\n"
     ]
    }
   ],
   "source": [
    "def write_gmt(path, tf_library):\n",
    "  with open(path, \"w\") as file:\n",
    "    for signature, geneset in tqdm.tqdm(tf_library.items()):\n",
    "      file.write(f\"{signature}\\t\\t\")\n",
    "      for gene in geneset:\n",
    "        file.write(f\"{gene}\\t\")\n",
    "      file.write(\"\\n\")\n",
    "\n",
    "write_gmt(f\"{output}/filtered_tfsets.gmt\", filtered_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create TF-TF edge list \n",
    "### Count the number of times each TF-TF interaction occurs.\n",
    "* Create the transpose of the GMT  where each line is a TF followed by the gene sets in which it is ranked (`transpose_human.gmt`)\n",
    "* For a given highly ranked TF, find all sets where it is present. Mark a source-target edge with any TFs present in the same set.\\\n",
    "* **Directionality** of the edge is determined by the sign (up/down) of the gene set where the relationship occurs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload high-enrichment TF GMT and create transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a GMT file formatted as \"signature id tf1 tf2 tf3 ... tf30\"\n",
    "\n",
    "def read_gmt2(path):\n",
    "  gmt = {}\n",
    "  print(\"Reading {}\".format(path))\n",
    "  with open(path, \"r\") as file:\n",
    "    for line in tqdm.tqdm(file):\n",
    "      # 'id' refers to up or down tag\n",
    "      signature, id, *tf = line.strip().split()\n",
    "      gmt[\" \".join([signature, id])] = set(tf)\n",
    "\n",
    "  return gmt\n",
    "\n",
    "def gmt_transpose(library):\n",
    "  tfs = list(set([tf for sublist in library.values() for tf in sublist]))\n",
    "  transpose = {}\n",
    "  for tf in tqdm.tqdm(tfs):\n",
    "    collect = list()\n",
    "    for signature in library:\n",
    "      if tf in library[signature]:\n",
    "        collect.append(signature)\n",
    "    transpose[tf] = set(collect)\n",
    "  return dict(transpose)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ./build_network_out/filtered_tfsets.gmt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29330it [00:00, 522036.46it/s]\n",
      "100%|██████████| 1588/1588 [00:02<00:00, 529.83it/s]\n"
     ]
    }
   ],
   "source": [
    "tf_sets = read_gmt2(f\"{output}/filtered_tfsets.gmt\")\n",
    "tf_transpose = gmt_transpose(tf_sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1588/1588 [00:00<00:00, 8334.78it/s]\n"
     ]
    }
   ],
   "source": [
    "def write_transpose_gmt(library, path):\n",
    "  with open(path, \"w\") as file:\n",
    "    for tf, signatures in tqdm.tqdm(library.items()):\n",
    "      file.write(f\"{tf}\\t\\t\")\n",
    "      for signature in signatures:\n",
    "        signature_join = \"-\".join(signature.split())\n",
    "        file.write(f\"{signature_join}\\t\")\n",
    "      file.write(\"\\n\")\n",
    "\n",
    "write_transpose_gmt(tf_transpose, f\"{output}/tf_transpose.gmt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_transpose(path):\n",
    "  gmt = {}\n",
    "  print(\"Reading {}\".format(path))\n",
    "  with open(path, \"r\") as file:\n",
    "    for line in tqdm.tqdm(file):\n",
    "        tf, *signature = line.strip().split()\n",
    "        gmt[tf] = set(signature)\n",
    "  return gmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading raw_data/human-geo-auto.gmt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "171441it [00:36, 4667.17it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ./build_network_out/tf_transpose.gmt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1588it [00:00, 23971.25it/s]\n"
     ]
    }
   ],
   "source": [
    "geo_gmt = read_gmt2(\"raw_data/human-geo-auto.gmt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store counts in a nested dictionary\\\n",
    "Dimensions: source_tf --> target_tf --> direction (up/down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1588/1588 [00:32<00:00, 48.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number below ten: \n",
      "\tup: 2333187\n",
      "\tdn: 2337917\n",
      "\twith both below ten: 2298226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1588/1588 [00:36<00:00, 43.55it/s]\n"
     ]
    }
   ],
   "source": [
    "high_tfs = list(tf_transpose.keys())\n",
    "\n",
    "# Create dict\n",
    "edge_matrix = {source : {target : {\n",
    "  \"+\": 0,\n",
    "  \"-\": 0\n",
    "} for target in high_tfs} for source in high_tfs}\n",
    "\n",
    "# Calculate counts\n",
    "for source in tqdm.tqdm(high_tfs):\n",
    "  for signature in tf_transpose[source]:\n",
    "\n",
    "    spl = signature.rsplit(\"-\", 1)\n",
    "    dir = \"+\" if spl[1] == \"up\" else \"-\" # \"dn\"\n",
    "    joined_sig = \" \".join(spl)\n",
    "\n",
    "    # if the gene set name is in the keys for the rummaGEO GMT\n",
    "    if joined_sig in geo_gmt.keys():\n",
    "      \n",
    "      # for each TF in the list of all TFs\n",
    "      for target in high_tfs:\n",
    "        \n",
    "        if target in geo_gmt[joined_sig]: # TF is in list of DEGs\n",
    "          edge_matrix[source][target][dir] += 1\n",
    "   \n",
    "    else:\n",
    "      raise Exception(\"Signature {} not found\".format(joined_sig))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of edges: 5043488\n",
      "Number of edges below ten: \n",
      "\tup: 2333187\n",
      "\tdn: 2337917\n",
      "Sources lost: 860\n",
      "Sources remaining: 728\n"
     ]
    }
   ],
   "source": [
    "# Count the number of True values in the dictionary\n",
    "below_ten_up_counts = 0\n",
    "below_ten_dn_counts = 0\n",
    "\n",
    "filtered_edge_matrix = defaultdict(lambda: defaultdict(lambda: defaultdict(dict)))\n",
    "\n",
    "# iterate through each source, filtering edges and counting how many are removed\n",
    "for source, target_list in edge_matrix.items():\n",
    "\n",
    "    # for each target find the counts\n",
    "    for target, count_dict in target_list.items():\n",
    "        for direction,count in count_dict.items():\n",
    "            # determine which counts are below 10\n",
    "            if direction == '+':\n",
    "                if count < 10:\n",
    "                    below_ten_up_counts += 1\n",
    "                else:\n",
    "                    filtered_edge_matrix[source][target][direction] = count\n",
    "            if direction == '-':\n",
    "                if count < 10:\n",
    "                    below_ten_dn_counts += 1\n",
    "                else:\n",
    "                    filtered_edge_matrix[source][target][direction] = count\n",
    "\n",
    "\n",
    "# sum the length of each dictionary for each source in edge-mat, multiply by 2\n",
    "total_edges = 2 * sum(\n",
    "    len(targets) for targets in edge_matrix.values() \n",
    ")\n",
    "\n",
    "print(f\"Total number of edges: {total_edges}\")\n",
    "print(\"Number of edges below ten: \")\n",
    "print(f\"\\tup: {below_ten_up_counts}\")\n",
    "print(f\"\\tdn: {below_ten_dn_counts}\")\n",
    "print(f\"Sources lost: {len(edge_matrix.keys()) - (len(filtered_edge_matrix.keys()))}\")\n",
    "print(f\"Sources remaining: {(len(filtered_edge_matrix.keys()))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flatten results into pandas DataFrame object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/728 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 728/728 [00:06<00:00, 115.19it/s]\n"
     ]
    }
   ],
   "source": [
    "s = filtered_edge_matrix.keys()\n",
    "t = set()\n",
    "[t.update(targs.keys()) for targs in filtered_edge_matrix.values()]\n",
    "\n",
    "targets = list(t)\n",
    "\n",
    "hindex = pd.MultiIndex.from_product([s, t, [\"+\", \"-\"]],\n",
    "  names = [\"source\", \"target\", \"direction\"])\n",
    "df = pd.DataFrame(index = hindex, columns = [\"count\"])\n",
    "\n",
    "for source, targets in tqdm.tqdm(filtered_edge_matrix.items()):\n",
    "  for target, data in targets.items():\n",
    "    for dir, count in data.items():\n",
    "      df.loc[(source, target, dir)] = count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove edges with empty counts and save results as a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df.dropna(subset=['count'])\n",
    "\n",
    "df_cleaned.to_csv(f\"{output}/edge_list_unfiltered.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional - visualize count distributions\n",
    "### Generates three log-scale histograms of TF-TF interaction counts: all counts, negative counts, and positive counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_cleaned' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 18\u001b[0m\n\u001b[1;32m     14\u001b[0m         plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNegative interaction counts\u001b[39m\u001b[38;5;124m'\u001b[39m, fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# get count values\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m all_counts \u001b[38;5;241m=\u001b[39m \u001b[43mdf_cleaned\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdropna()\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Filter the DataFrame for rows where the direction is \"+\"\u001b[39;00m\n\u001b[1;32m     20\u001b[0m positive_counts \u001b[38;5;241m=\u001b[39m df_cleaned\u001b[38;5;241m.\u001b[39mloc[(\u001b[38;5;28mslice\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m), \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m+\u001b[39m\u001b[38;5;124m\"\u001b[39m), :][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_cleaned' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot log-scaled histogram of counts\n",
    "\n",
    "def tf_histogram(name, counts, num_bins=300, fig_size=(10,6)):\n",
    "    plt.figure(figsize=fig_size)\n",
    "    plt.hist(counts, bins = num_bins, edgecolor ='none')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Interaction count', fontsize=14)\n",
    "    plt.ylabel('Frequency', fontsize=14)\n",
    "    if name == \"all\":\n",
    "        plt.title('All TF-TF interaction counts', fontsize=16)\n",
    "    elif name == \"pos\":\n",
    "        plt.title('Positive interaction counts', fontsize=16)\n",
    "    elif name == \"neg\": \n",
    "        plt.title('Negative interaction counts', fontsize=16)\n",
    "\n",
    "\n",
    "# get count values\n",
    "all_counts = df_cleaned['count'].dropna()\n",
    "# Filter the DataFrame for rows where the direction is \"+\"\n",
    "positive_counts = df_cleaned.loc[(slice(None), slice(None), \"+\"), :]['count']\n",
    "# Filter the DataFrame for rows where the direction is \"+\"\n",
    "negative_counts = (df_cleaned.loc[(slice(None), slice(None), \"-\"), :])['count']\n",
    "\n",
    "tf_histogram(\"all\", all_counts)\n",
    "plt.savefig(f\"{output}/img/all_counts_histo.png\")\n",
    "plt.show()\n",
    "\n",
    "tf_histogram(\"pos\", positive_counts)\n",
    "plt.savefig(f\"{output}/img/pos_counts_histo.png\")\n",
    "plt.show()\n",
    "\n",
    "tf_histogram(\"neg\", negative_counts)\n",
    "plt.savefig(f\"{output}/img/neg_counts_histo.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
