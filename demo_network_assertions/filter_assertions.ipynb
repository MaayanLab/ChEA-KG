{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter TF network edges by expected counts\n",
    "### Describes a regimen to filter human TF interactions in the edge list produced by `build_edge_list.ipynb`\n",
    "### The second part of this notebook formats the edge list into neo4j assertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "import statistics \n",
    "import scipy.stats as stats\n",
    "import tqdm\n",
    "from itertools import islice\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = './build_network_out'\n",
    "output = './filter_assertions_out'\n",
    "raw_data = './raw_data'\n",
    "assertions_dir = './kg_assertions_for_neo4j'\n",
    "benchmark_folder = output + '/benchmarking'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload edge matrix to a dataframe and format it as a multiindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_file = f\"{output}/unfiltered_edge_list.csv\"\n",
    "\n",
    "initial_counts = pd.read_csv(initial_file)\n",
    "\n",
    "initial_sources = initial_counts['source'].unique()\n",
    "initial_targets = initial_counts['target'].unique()\n",
    "\n",
    "initial_counts.set_index([\"source\", \"target\", \"direction\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test expected counts methods\n",
    "#### Choose an expected counts method, then proceed to filter results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Shuffle signature sets between source TFs and recalculate counts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload signature sets which are enriched by each TF. Produces a dictionary formatted as `tf:[signature1 ... signatureN]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_method = 'signature_shuffling'\n",
    "N_TRIALS = 50\n",
    "\n",
    "# read in tf_transpose.gmt and upload it into an edge list\n",
    "# upload humanedge.csv to a dictionary\n",
    "\n",
    "signature_file = f\"{input}/tf_transpose.gmt\"\n",
    "signature_sets = defaultdict(set)\n",
    "\n",
    "\n",
    "with open(signature_file, \"r\") as file:\n",
    "    for line in tqdm.tqdm(file):\n",
    "        tf, *signature = line.strip().split()\n",
    "        signature_sets[tf] = set(signature)\n",
    "    \n",
    "\n",
    "# now shuffle sets between terms\n",
    "def shuffle_signatures(signature_sets):\n",
    "    tfs = list(signature_sets.keys())\n",
    "    sigs = list(signature_sets.values())\n",
    "\n",
    "    random.shuffle(sigs)\n",
    "\n",
    "    shuffled_sigsets = dict(zip(tfs, sigs))\n",
    "    \n",
    "    return shuffled_sigsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload RummaGEO gene sets to produce counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_gmt2(path):\n",
    "  gmt = {}\n",
    "  print(\"Reading {}\".format(path))\n",
    "  with open(path, \"r\") as file:\n",
    "    for line in tqdm.tqdm(file):\n",
    "      # 'id' refers to up or down tag\n",
    "      signature, id, *tf = line.strip().split()\n",
    "      gmt[\" \".join([signature, id])] = set(tf)\n",
    "\n",
    "  return gmt\n",
    "\n",
    "geo_gmt = read_gmt2(f\"{raw_data}/human-geo-auto.gmt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Recalculate counts\n",
    "Produce an edge list using the randomly shuffled signature sets. Repeat several times and store counts. \n",
    "\n",
    "Produces a dictionary formatted \n",
    "`\n",
    "{ source: { \n",
    "    target: { \n",
    "            '+': np.array(up_counts_all_trials)\n",
    "            '-': np.array(dn_counts_all_trials)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dict\n",
    "all_high_tfs = list(signature_sets.keys())\n",
    "\n",
    "expected_counts = {source : {target : {\n",
    "    \"+\": np.zeros(N_TRIALS),\n",
    "    \"-\": np.zeros(N_TRIALS),\n",
    "  } for target in all_high_tfs} for source in all_high_tfs}\n",
    "\n",
    "for i in tqdm.tqdm(range(N_TRIALS)):\n",
    "  shuffled_sets = shuffle_signatures(signature_sets)\n",
    "\n",
    "  # Calculate counts\n",
    "  for source in all_high_tfs:\n",
    "    for signature in shuffled_sets[source]:\n",
    "\n",
    "      spl = signature.rsplit(\"-\", 1)\n",
    "      dir = \"+\" if spl[1] == \"up\" else \"-\" # \"dn\"\n",
    "      joined_sig = \" \".join(spl)\n",
    "\n",
    "      # if the gene set name is in the keys for the rummaGEO GMT\n",
    "      if joined_sig in geo_gmt.keys():\n",
    "        \n",
    "        # for each TF in the list of all TFs\n",
    "        for target in all_high_tfs:\n",
    "          \n",
    "          if target in geo_gmt[joined_sig]: # TF is in list of DEGs\n",
    "            expected_counts[source][target][dir][i] += 1\n",
    "    \n",
    "      else:\n",
    "        raise Exception(\"Signature {} not found\".format(joined_sig))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Randomly generate new edges by randomly selecting (weighted) sources and targets independently   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a randomly generated edge list by drawing source-target pairs weighted by their occurance in the initial dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_method = 'node_weighted'\n",
    "n_iterations = 50\n",
    "\n",
    "source_counts = initial_counts.groupby(['source']).sum()\n",
    "target_counts = initial_counts.groupby(['target']).sum()\n",
    "\n",
    "source_weights = source_counts['count'].tolist()\n",
    "target_weights = target_counts['count'].tolist()\n",
    "\n",
    "num_s_edges = sum(source_weights)\n",
    "num_t_edges = sum(target_weights)\n",
    "\n",
    "source_labels = source_counts.index.tolist()\n",
    "target_labels = target_counts.index.tolist()\n",
    "\n",
    "# initialize expected counts\n",
    "expected_counts = {source : {target : {\n",
    "    \"+\": np.zeros(n_iterations),\n",
    "    \"-\": np.zeros(n_iterations),\n",
    "  } for target in initial_targets} for source in initial_sources}\n",
    "\n",
    "for i in tqdm.tqdm(range(n_iterations)):\n",
    "        # Perform weighted random choice\n",
    "        random_source = random.choices(source_labels, weights=source_weights, k=num_s_edges)\n",
    "        random_target = random.choices(target_labels, weights=target_weights, k=num_t_edges)\n",
    "          \n",
    "        if len(random_source) != len(random_target):\n",
    "                print('Source length != target length')\n",
    "        else:\n",
    "                for source, target in zip(random_source, random_target):\n",
    "                        # randomly choose an edge direction\n",
    "                        direction = random.randint(0,1)\n",
    "                        if direction == 0:\n",
    "                                expected_counts[source][target][\"+\"][i] += 1\n",
    "                        else:\n",
    "                                expected_counts[source][target][\"-\"][i] += 1\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hindex = pd.MultiIndex.from_product([initial_sources, initial_targets, [\"+\", \"-\"]],\n",
    "  names = [\"source\", \"target\", \"direction\"])\n",
    "df = pd.DataFrame(index = hindex, columns = [\"count\"])\n",
    "\n",
    "iteration_columns = [f'count_{i}' for i in range(n_iterations)]\n",
    "df = pd.DataFrame(index=hindex, columns=iteration_columns)\n",
    "# Ensure the DataFrame index is sorted\n",
    "df.sort_index(inplace=True)\n",
    "\n",
    "# populate df\n",
    "for source in tqdm.tqdm(initial_sources):\n",
    "  for target in initial_targets:\n",
    "    for dir in ['+','-']:\n",
    "      df.loc[(source, target, dir),:] = expected_counts[source][target][dir]\n",
    "\n",
    "# optional - save random counts\n",
    "# df.to_csv(f'randomcounts_{benchmark_method}.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3: Randomly generate an edge list using weighted pairs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [14:21<00:00, 17.22s/it]\n"
     ]
    }
   ],
   "source": [
    "benchmark_method = 'edge_weighted'\n",
    "n_iterations = 50\n",
    "\n",
    "def find_random_edges():\n",
    "        # get the weights\n",
    "        edge_counts = initial_counts.groupby(['source', 'target']).sum()\n",
    "        edge_weights = edge_counts['count'].tolist()\n",
    "\n",
    "        # get the total number of edges counted\n",
    "        num_edges = sum(edge_weights)\n",
    "\n",
    "        # get the labels in the same order as their weights\n",
    "        edge_labels = edge_counts.index.tolist()\n",
    "\n",
    "        # randomly select the same number of sources as in the original edge list\n",
    "        random_edges = random.choices(edge_labels, weights=edge_weights, k=num_edges)\n",
    "\n",
    "        return random_edges\n",
    "\n",
    "# initialize expected counts\n",
    "expected_counts = {source : {target : {\n",
    "    \"+\": np.zeros(n_iterations),\n",
    "    \"-\": np.zeros(n_iterations),\n",
    "  } for target in initial_targets} for source in initial_sources}\n",
    "\n",
    "for i in tqdm.tqdm(range(n_iterations)):\n",
    "        # Perform weighted random choice\n",
    "        random_edges = find_random_edges()     \n",
    "\n",
    "        for index in random_edges:\n",
    "                # extract the source, target and randomly assign a direction\n",
    "                source, target = index\n",
    "\n",
    "                # randomly choose an edge direction\n",
    "                direction = random.randint(0,1)\n",
    "\n",
    "                if direction == 0:\n",
    "                        expected_counts[source][target][\"+\"][i] += 1\n",
    "                else:\n",
    "                        expected_counts[source][target][\"-\"][i] += 1\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter results using expected counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate edge statistics - observed counts, expected counts, mean, stdev, z-score, and p-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 728/728 [02:12<00:00,  5.51it/s]\n"
     ]
    }
   ],
   "source": [
    "# make a similar array to store stats\n",
    "hindex = pd.MultiIndex.from_product([initial_sources, initial_targets,['+', '-']],\n",
    "  names = [\"source\", \"target\", \"relation\"])\n",
    "edge_statistics = pd.DataFrame(index = hindex, columns = [\"observed\", \"expected\", \"expected stdev\", \"z-score\", \"p-value\"])\n",
    "\n",
    "\n",
    "for source in tqdm.tqdm(initial_sources):\n",
    "  for target in initial_targets:\n",
    "    for dir in ['+','-']:\n",
    "      trial_counts = expected_counts[source][target][dir]\n",
    "\n",
    "      # find expected and observed counts -- if the key doesn't exist, then the observed counts are zero\n",
    "      try:\n",
    "        obsv_counts = initial_counts.loc[(source, target, dir), 'count']\n",
    "      except KeyError:\n",
    "        obsv_counts = 0\n",
    "\n",
    "      # calculate statistics \n",
    "      mean = statistics.mean(trial_counts)\n",
    "      stdev = statistics.stdev(trial_counts)\n",
    "\n",
    "      # ignore values with no stdev and obsv counts == 0\n",
    "      if stdev != 0:\n",
    "        z_score = (obsv_counts - mean) / (stdev)\n",
    "        p_value = 1 - stats.norm.cdf(z_score)\n",
    "\n",
    "        # store in dataframe, removing non-existent edges\n",
    "        if obsv_counts > 0:\n",
    "          edge_statistics.loc[(source, target, dir)] = [obsv_counts, mean, stdev, z_score, p_value]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove insignificant edges.\\\n",
    "*Z-score is the chosen method because it allows for finer filtering. Uncomment p-vaue as desired.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop NAs\n",
    "edge_statistics.dropna(inplace=True)\n",
    "\n",
    "z_sorted = edge_statistics.sort_values(by='z-score', ascending = False)\n",
    "# p_sorted = edge_statistics.sort_values(by='p-value')\n",
    "\n",
    "Z_MIN = 2.325\n",
    "# P_MIN = 1e-12\n",
    "\n",
    "# OR - optional - read in from file: \n",
    "# benchmark_method = 'signature_shuffling'\n",
    "# edge_statistics = pd.read_csv(f'/Users/anna/Projects/KG_UI/build_TF_network/filter_assertions_out/benchmarking/{benchmark_method}/{benchmark_method}_z_sorted_edge_stats.csv', delimiter = '\\t')\n",
    "\n",
    "edge_statistics.set_index(['source', 'target','relation'], inplace=True)\n",
    "\n",
    "significant_edges = edge_statistics.loc[((edge_statistics['z-score'] != float('Inf')) & (edge_statistics['z-score'] > Z_MIN))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For pairs that have significant edges in both directions, keep only the edge with the most significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# work from a copy since we're removing entries directly\n",
    "significant_edges_copy = significant_edges.copy()\n",
    "direction_edges_to_drop = []\n",
    "\n",
    "# search source-target pairs and retain only the edge with the highest significance \n",
    "for (source, target), group in significant_edges_copy.groupby(level=['source', 'target']):\n",
    "\n",
    "    directions = group.index.get_level_values('relation')\n",
    "    up_exists = '+' in directions\n",
    "    dn_exists = '-' in directions\n",
    "\n",
    "    # remove either up or down if both are significant\n",
    "    if up_exists and dn_exists:\n",
    "        # Filter to get '+' and '-' entries\n",
    "        up_data = group.loc[(slice(None), slice(None), '+'), :]\n",
    "        dn_data = group.loc[(slice(None), slice(None), '-'), :]   \n",
    "\n",
    "        # remove lower z-score\n",
    "        if up_data['z-score'].values[0] < dn_data['z-score'].values[0]:\n",
    "          direction_edges_to_drop.append((source, target, '+')) \n",
    "        else:\n",
    "          direction_edges_to_drop.append((source, target, '-'))\n",
    "\n",
    "significant_edges_copy.drop(direction_edges_to_drop, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only the top 3 most significant edges from each source \n",
    "max_src_edges = 3\n",
    "significant_edges_copy.sort_values(by=['source','z-score'], inplace=True)\n",
    "final_edge_list = significant_edges_copy.groupby(level='source').head(max_src_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPTIONAL: saved output and histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save collected statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_sorted.to_csv(f\"{benchmark_folder}/{benchmark_method}_z_sorted_edge_stats.csv\", sep='\\t')\n",
    "# p_sorted.to_csv(f\"{output}/p_sorted_edge_statistics\", sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save edge list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_edge_list.to_csv(f\"{benchmark_folder}/{benchmark_method}/edge_list_filtered.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot histogram of edge counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot log-scaled histogram of counts\n",
    "\n",
    "def tf_histogram(name, counts, num_bins=300, fig_size=(10,6)):\n",
    "    plt.figure(figsize=fig_size)\n",
    "    plt.hist(counts, bins = num_bins, edgecolor ='none')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Interaction count', fontsize=14)\n",
    "    plt.ylabel('Frequency', fontsize=14)\n",
    "    if name == \"all\":\n",
    "        plt.title('All TF-TF interaction counts', fontsize=16)\n",
    "    elif name == \"pos\":\n",
    "        plt.title('Positive interaction counts', fontsize=16)\n",
    "    elif name == \"neg\": \n",
    "        plt.title('Negative interaction counts', fontsize=16)\n",
    "\n",
    "\n",
    "# get count values\n",
    "all_counts = final_edge_list['observed']\n",
    "# Filter the DataFrame for rows where the direction is \"+\"\n",
    "positive_counts = final_edge_list.loc[(slice(None), slice(None), \"+\"), :]['observed']\n",
    "# Filter the DataFrame for rows where the direction is \"+\"\n",
    "negative_counts = final_edge_list.loc[(slice(None), slice(None), \"-\"), :]['observed']\n",
    "\n",
    "tf_histogram(\"all\", all_counts)\n",
    "plt.savefig(f\"{output}/img/all_histo_filtered_weighted_shuffling.png\")\n",
    "plt.show()\n",
    "\n",
    "tf_histogram(\"pos\", positive_counts)\n",
    "plt.savefig(f\"{output}/img/pos_histo_filtered_weighted_shuffling.png\")\n",
    "plt.show()\n",
    "\n",
    "tf_histogram(\"neg\", negative_counts)\n",
    "plt.savefig(f\"{output}/img/neg_histo_filtered_weighted_shuffling.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format data for UI ingestion\n",
    "### Format nodes: [id,label]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the label used to describe the node type\n",
    "node_name = \"Transcription Factor\"\n",
    "\n",
    "# collect all unique source and target nodes\n",
    "nodes = set() \n",
    "for (source, target), group in final_edge_list.groupby(level=['source', 'target']):\n",
    "    nodes.add((source, source))\n",
    "    nodes.add((target, target))\n",
    "\n",
    "\n",
    "# convert to df for easy csv formatting\n",
    "node_df = pd.DataFrame(list(nodes), columns=['id', 'label'])\n",
    "node_df.to_csv(f'{benchmark_folder}/{benchmark_method}/{node_name}.nodes.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format edges: [source,relation,target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder the index to match ingestion format\n",
    "new_index = ['source','relation','target']\n",
    "index_frame = final_edge_list.index.to_frame()\n",
    "index_frame = index_frame[new_index]\n",
    "\n",
    "# rename relations to be more descriptive\n",
    "relation_rename = {\n",
    "    '+': 'upregulates',\n",
    "    '-': 'downregulates'\n",
    "}\n",
    "index_frame['relation'] = index_frame['relation'].replace(relation_rename)\n",
    "\n",
    "# split the edge list based on relation type and save to two files\n",
    "relation_types = index_frame['relation'].unique()\n",
    "\n",
    "for relation in relation_types:\n",
    "    filtered_df = index_frame[index_frame['relation'] == relation]\n",
    "    file_name = f\"{benchmark_folder}/{benchmark_method}/{node_name}.{relation}.{node_name}.edges.csv\"\n",
    "    filtered_df.to_csv(file_name, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
