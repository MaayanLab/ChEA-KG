{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter TF network edges by expected counts\n",
    "### Describes a regimen to filter human TF interactions in the edge list produced by `build_edge_list.ipynb`\n",
    "### The second part of this notebook formats the edge list into neo4j assertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "import statistics \n",
    "import scipy.stats as stats\n",
    "import tqdm\n",
    "from itertools import islice\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = './build_edge_list_out'\n",
    "output = './filter_assertions_out'\n",
    "raw_data = './raw_data'\n",
    "assertions_dir = './kg_assertions_for_neo4j'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle signature sets between source TFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload signature sets which are enriched by each TF. Produces a dictionary formatted as `tf:[signature1 ... signatureN]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in tf_transpose.gmt and upload it into an edge list\n",
    "# upload humanedge.csv to a dictionary\n",
    "\n",
    "signature_file = f\"{input}/tf_transpose.gmt\"\n",
    "signature_sets = defaultdict(set)\n",
    "\n",
    "\n",
    "with open(signature_file, \"r\") as file:\n",
    "    for line in tqdm.tqdm(file):\n",
    "        tf, *signature = line.strip().split()\n",
    "        signature_sets[tf] = set(signature)\n",
    "    \n",
    "\n",
    "# now shuffle sets between terms\n",
    "def shuffle_signatures(signature_sets):\n",
    "    tfs = list(signature_sets.keys())\n",
    "    sigs = list(signature_sets.values())\n",
    "\n",
    "    random.shuffle(sigs)\n",
    "\n",
    "    shuffled_sigsets = dict(zip(tfs, sigs))\n",
    "    \n",
    "    return shuffled_sigsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload RummaGEO gene sets to produce counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_gmt2(path):\n",
    "  gmt = {}\n",
    "  print(\"Reading {}\".format(path))\n",
    "  with open(path, \"r\") as file:\n",
    "    for line in tqdm.tqdm(file):\n",
    "      # 'id' refers to up or down tag\n",
    "      signature, id, *tf = line.strip().split()\n",
    "      gmt[\" \".join([signature, id])] = set(tf)\n",
    "\n",
    "  return gmt\n",
    "\n",
    "geo_gmt = read_gmt2(f\"{raw_data}/human-geo-auto.gmt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recalculate counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Produce an edge list using the randomly shuffled signature sets. Repeat several times and store counts. \n",
    "\n",
    "Produces a dictionary formatted \n",
    "`\n",
    "{ source: { \n",
    "    target: { \n",
    "            '+': np.array(up_counts_all_trials)\n",
    "            '-': np.array(dn_counts_all_trials)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRIALS = 50\n",
    "\n",
    "# Create dict\n",
    "all_high_tfs = list(signature_sets.keys())\n",
    "\n",
    "expected_counts = {source : {target : {\n",
    "    \"+\": np.zeros(N_TRIALS),\n",
    "    \"-\": np.zeros(N_TRIALS),\n",
    "  } for target in all_high_tfs} for source in all_high_tfs}\n",
    "\n",
    "for i in tqdm.tqdm(range(N_TRIALS)):\n",
    "  shuffled_sets = shuffle_signatures(signature_sets)\n",
    "\n",
    "  # Calculate counts\n",
    "  for source in all_high_tfs:\n",
    "    for signature in shuffled_sets[source]:\n",
    "\n",
    "      spl = signature.rsplit(\"-\", 1)\n",
    "      dir = \"+\" if spl[1] == \"up\" else \"-\" # \"dn\"\n",
    "      joined_sig = \" \".join(spl)\n",
    "\n",
    "      # if the gene set name is in the keys for the rummaGEO GMT\n",
    "      if joined_sig in geo_gmt.keys():\n",
    "        \n",
    "        # for each TF in the list of all TFs\n",
    "        for target in all_high_tfs:\n",
    "          \n",
    "          if target in geo_gmt[joined_sig]: # TF is in list of DEGs\n",
    "            expected_counts[source][target][dir][i] += 1\n",
    "    \n",
    "      else:\n",
    "        raise Exception(\"Signature {} not found\".format(joined_sig))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter by significance of counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload observed counts from initially filtered edge list into a dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload humanedge.csv to a dictionary\n",
    "obs_file = f\"{input}/edge_list_unfiltered.csv\"\n",
    "observed_counts = {source : {target : {\n",
    "    \"+\": 0,\n",
    "    \"-\": 0,\n",
    "  } for target in all_high_tfs} for source in all_high_tfs}\n",
    "\n",
    "with open(obs_file, 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader) # skip header\n",
    "    for source,target,direction,count in reader:\n",
    "        if direction == \"+\" or direction == \"-\":\n",
    "            observed_counts[source][target][direction] = int(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a similar array to store stats. this will introduce some edges that don't exist -- we'll remove these later\n",
    "hindex = pd.MultiIndex.from_product([all_high_tfs, all_high_tfs, ['+', '-']],\n",
    "  names = [\"source\", \"target\", \"direction\"])\n",
    "edge_statistics = pd.DataFrame(index = hindex, columns = [\"observed\", \"expected\", \"expected stdev\", \"z-score\", \"p-value\"])\n",
    "\n",
    "for source, targets in tqdm.tqdm(expected_counts.items()):\n",
    "  for target, data in targets.items():\n",
    "    for dir, counts in data.items():\n",
    "      # find expected and observed counts \n",
    "      obsv_counts = observed_counts[source][target][dir]\n",
    "\n",
    "      # calculate statistics \n",
    "      mean = statistics.mean(counts)\n",
    "      stdev = statistics.stdev(counts)\n",
    "      if stdev != 0:\n",
    "        z_score = (obsv_counts - mean) / (stdev)\n",
    "        p_value = 1 - stats.norm.cdf(z_score)\n",
    "      else:\n",
    "        z_score = float('inf')\n",
    "        p_value = 2\n",
    "\n",
    "      # store in dataframe, removing non-existent edges\n",
    "      if obsv_counts > 0:\n",
    "        edge_statistics.loc[(source, target, dir)] = [obsv_counts, mean, stdev, z_score, p_value]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove insignificant edges. For pairs that have significant edges in both directions, keep only the edge with the most significance.\\\n",
    "*Z-score is the chosen method because it allows for finer filtering. Uncomment p-vaue as desired.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_sorted = edge_statistics.sort_values(by='z-score')\n",
    "# p_sorted = edge_statistics.sort_values(by='p-value')\n",
    "\n",
    "Z_MIN = 13.0\n",
    "# P_MIN = 1e-12\n",
    "\n",
    "# work from a copy since we're removing entries directly\n",
    "significant_edges = edge_statistics.loc[((edge_statistics['z-score'] != float('Inf')) & (edge_statistics['z-score'] > Z_MIN))]\n",
    "final_edge_list = significant_edges.copy()\n",
    "edges_to_drop = []\n",
    "\n",
    "# search source-target pairs and retain only the edge with the highest significance \n",
    "for (source, target), group in final_edge_list.groupby(level=['source', 'target']):\n",
    "\n",
    "    directions = group.index.get_level_values('direction')\n",
    "    up_exists = '+' in directions\n",
    "    dn_exists = '-' in directions\n",
    "\n",
    "    if up_exists and dn_exists:\n",
    "        # Filter to get '+' and '-' entries\n",
    "        up_data = group.loc[(slice(None), slice(None), '+'), :]\n",
    "        dn_data = group.loc[(slice(None), slice(None), '-'), :]   \n",
    "\n",
    "        # remove lower z-score\n",
    "        if up_data['z-score'].values[0] < dn_data['z-score'].values[0]:\n",
    "          edges_to_drop.append((source, target, '+')) \n",
    "        else:\n",
    "          edges_to_drop.append((source, target, '-'))\n",
    "\n",
    "final_edge_list.drop(index=edges_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPTIONAL: saved output and histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save collected statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_sorted.to_csv(f\"{output}/z_sorted_edge_stats.csv\", sep='\\t')\n",
    "# p_sorted.to_csv(f\"{output}/p_sorted_edge_statistics\", sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save edge list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_edge_list.to_csv(f\"{output}/edge_list_filtered.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot histogram of edge counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot log-scaled histogram of counts\n",
    "\n",
    "def tf_histogram(name, counts, num_bins=300, fig_size=(10,6)):\n",
    "    plt.figure(figsize=fig_size)\n",
    "    plt.hist(counts, bins = num_bins, edgecolor ='none')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Interaction count', fontsize=14)\n",
    "    plt.ylabel('Frequency', fontsize=14)\n",
    "    if name == \"all\":\n",
    "        plt.title('All TF-TF interaction counts', fontsize=16)\n",
    "    elif name == \"pos\":\n",
    "        plt.title('Positive interaction counts', fontsize=16)\n",
    "    elif name == \"neg\": \n",
    "        plt.title('Negative interaction counts', fontsize=16)\n",
    "\n",
    "\n",
    "# get count values\n",
    "all_counts = final_edge_list['observed']\n",
    "# Filter the DataFrame for rows where the direction is \"+\"\n",
    "positive_counts = final_edge_list.loc[(slice(None), slice(None), \"+\"), :]['observed']\n",
    "# Filter the DataFrame for rows where the direction is \"+\"\n",
    "negative_counts = final_edge_list.loc[(slice(None), slice(None), \"-\"), :]['observed']\n",
    "\n",
    "tf_histogram(\"all\", all_counts)\n",
    "plt.savefig(f\"{output}/img/all_histo_filtered.png\")\n",
    "plt.show()\n",
    "\n",
    "tf_histogram(\"pos\", positive_counts)\n",
    "plt.savefig(f\"{output}/img/pos_histo_filtered.png\")\n",
    "plt.show()\n",
    "\n",
    "tf_histogram(\"neg\", negative_counts)\n",
    "plt.savefig(f\"{output}/img/neg_histo_filtered.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format data for UI ingestion\n",
    "### Format nodes: [id,label]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the label used to describe the node type\n",
    "node_name = \"Transcription Factor\"\n",
    "\n",
    "# collect all unique source and target nodes\n",
    "nodes = set() \n",
    "for (source, target), group in final_edge_list.groupby(level=['source', 'target']):\n",
    "    nodes.add((source, source))\n",
    "    nodes.add((target, target))\n",
    "\n",
    "\n",
    "# convert to df for easy csv formatting\n",
    "node_df = pd.DataFrame(list(nodes), columns=['id', 'label'])\n",
    "node_df.to_csv(f'{assertions_dir}/{node_name}.nodes.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format edges: [source,relation,target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder the index to match ingestion format\n",
    "new_index = ['source','relation','target']\n",
    "final_edge_list.reindex(index=new_index)\n",
    "index = final_edge_list.index.to_frame()\n",
    "\n",
    "# rename relations to be more descriptive\n",
    "relation_rename = {\n",
    "    '+': 'upregulates',\n",
    "    '-': 'downregulates'\n",
    "}\n",
    "final_edge_list['relation'] = final_edge_list['relation'].replace(relation_rename)\n",
    "\n",
    "# split the edge list based on relation type and save to two files\n",
    "relation_types = final_edge_list['relation'].unique()\n",
    "\n",
    "for relation in relation_types:\n",
    "    filtered_df = final_edge_list[final_edge_list['relation'] == relation]\n",
    "    file_name = f\"{assertions_dir}/{node_name}.{relation}.{node_name}.edges.csv\"\n",
    "    filtered_df.to_csv(file_name, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
